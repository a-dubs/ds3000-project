{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All player stats variables: kills, hskills, assists, fassists, deaths, kdratio, kddiff, adr, fkdiff, rating\n",
      "SELECTED player stats variables: kills, kdratio, rating\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set() \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from alive_progress import alive_bar, alive_it \n",
    "import random\n",
    "\n",
    "\n",
    "matches_dataset = pd.read_csv(\"all_matches_dataset.csv\")\n",
    "matches_dataset\n",
    "\n",
    "PLAYER_STAT_VARIABLES = [var.replace(\"t1p1_\",\"\") for var in list(matches_dataset.columns) if (\"t1p1_\" in var) and (not \"player\" in var)]\n",
    "print(\"All player stats variables:\", \", \".join(PLAYER_STAT_VARIABLES))\n",
    "\n",
    "many_pstat_vars = [\"kills\", \"assists\", \"deaths\", \"kdratio\", \"kddiff\", \"adr\", \"fkdiff\", \"rating\"]\n",
    "few_pstat_vars = [\"kills\", \"kdratio\", \"kddiff\", \"fkdiff\", \"rating\"]\n",
    "min_pstat_vars = [\"kills\", \"kdratio\", \"rating\"]\n",
    "\n",
    "SELECTED_PSTAT_VARS = min_pstat_vars\n",
    "print(\"SELECTED player stats variables:\", \", \".join(SELECTED_PSTAT_VARS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_dataset_by_date(full_df, start_dt : datetime, stop_dt : datetime, date_col_name : str = \"match_date\"):\n",
    "    df : DataFrame = full_df.copy()\n",
    "    keep_dict = {}\n",
    "    if start_dt > stop_dt:\n",
    "        print(\"datetimes were given backwards\")\n",
    "        temp_dt = start_dt\n",
    "        start_dt = stop_dt\n",
    "        stop_dt = temp_dt\n",
    "    for index, row in df.iterrows():\n",
    "        if not((dt:=datetime.strptime(row[date_col_name], '%Y-%m-%d')) < start_dt or dt >= stop_dt):\n",
    "            # keep_indeces.append(index)\n",
    "            # keep_rows.append(row)\n",
    "            keep_dict[index] = row \n",
    "    return DataFrame.from_dict(keep_dict,orient=\"index\")\n",
    "\n",
    "\n",
    "def partition_dataset_by_date(df : DataFrame, stop_date = \"2017-08-01\", partition_length : int = 120, overlapping : bool = True, partition_shift : int = 30, pstat_vars_filter = None) -> Series:\n",
    "    partitions = {}\n",
    "    stop_dt = datetime.strptime(stop_date, '%Y-%m-%d')\n",
    "    dt_it = datetime.now()\n",
    "    if not overlapping:\n",
    "        partition_shift = partition_length\n",
    "    print(\"Partitioning dataset temporally (by date):\")\n",
    "    while dt_it - timedelta(days=(partition_length)) > stop_dt:\n",
    "        partition = filter_dataset_by_date(df, date_col_name= \"match_date\", start_dt=dt_it - timedelta(days = partition_length), stop_dt=dt_it)\n",
    "        date_range = f\"{datetime.strftime(dt_it - timedelta(days = partition_length),'%Y-%m-%d')} -> {datetime.strftime(dt_it,'%Y-%m-%d')}\"\n",
    "        partitions[date_range] = partition\n",
    "        print(date_range,\":\",partition.shape[0],\"items in partition\")\n",
    "        dt_it = dt_it - timedelta(days=partition_shift)\n",
    "    return Series(partitions)\n",
    "\n",
    "\n",
    "def get_aggregate_players_stats(df : DataFrame, pstat_vars = SELECTED_PSTAT_VARS):\n",
    "    teams_pstats = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for tn in (1,2):\n",
    "            # score_diff = row[\"t1_total_rw\"] - row[\"t2_total_rw\"] if tn==1 else row[\"t2_total_rw\"] - row[\"t1_total_rw\"]\n",
    "            tname = row[f\"t{tn}_name\"]\n",
    "            \n",
    "            for pn in (1,2,3,4,5):\n",
    "                id = f\"t{tn}p{pn}_\"\n",
    "                pname = row[id+\"player\"]\n",
    "                if teams_pstats.get(tname) == None:\n",
    "                    teams_pstats[tname] = {}\n",
    "                if teams_pstats[tname].get(pname) == None:\n",
    "                    teams_pstats[tname][pname] = []\n",
    "                teams_pstats[tname][pname].append({var:row[id+var] for var in pstat_vars})\n",
    "    return teams_pstats\n",
    "\n",
    "\n",
    "def get_active_rosters(matches_df) -> dict[str, list[str]]:\n",
    "    aggregate_pstats = get_aggregate_players_stats(matches_df)\n",
    "    teams_active_players = {}\n",
    "    for tname in aggregate_pstats:\n",
    "        team = aggregate_pstats[tname]\n",
    "        players_games_played = {}\n",
    "        # players_games_played =  {player: for player in team}\n",
    "        for pname in team:\n",
    "            player = team[pname]\n",
    "            players_games_played[pname] = len(list(player))\n",
    "        active_players = list(pd.Series(players_games_played).sort_values(ascending=False).iloc[:5].index)\n",
    "        teams_active_players[tname] = active_players\n",
    "    return teams_active_players\n",
    "\n",
    "\n",
    "def get_sorted_teams_rosters(df : DataFrame, rosters, mode = \"mean\"):\n",
    "    aggregated_teams_pstats = get_aggregate_players_stats(df)\n",
    "    sorted_rosters = {}\n",
    "    for tname in rosters:\n",
    "        team = aggregated_teams_pstats[tname]\n",
    "        pn = 0\n",
    "        pratings = {}\n",
    "        for pname in rosters[tname]:\n",
    "            pmatches = pd.DataFrame(team[pname])\n",
    "            prating_stats = pmatches.loc[:,[\"rating\"]].describe().squeeze()\n",
    "            prating_agg_val = prating_stats[mode]\n",
    "            pratings[pname] = prating_agg_val\n",
    "        \n",
    "        sorted_rosters[tname] = list(Series(pratings).sort_values(ascending=False).index)\n",
    "    return sorted_rosters\n",
    "\n",
    "# active_rosters = get_active_rosters(recent_matches_dataset)\n",
    "\n",
    "\n",
    "def make_teams_profiles(df : DataFrame, pstat_vars = SELECTED_PSTAT_VARS, mode = \"mean\"):\n",
    "    \n",
    "    rosters = get_sorted_teams_rosters(df, rosters=get_active_rosters(df))\n",
    "\n",
    "    aggregated_teams_pstats = get_aggregate_players_stats(df, pstat_vars=pstat_vars)\n",
    "\n",
    "    teams_profiles = {}\n",
    "    for tname in rosters:\n",
    "        team = aggregated_teams_pstats[tname]\n",
    "        team_flattened_pstats = {}\n",
    "        pn = 0\n",
    "        for pname in rosters[tname]:\n",
    "            pn += 1\n",
    "            pmatches = pd.DataFrame(team[pname])\n",
    "            pmatches_stats = pmatches.describe()\n",
    "            pstats : Series = pmatches_stats.loc[[mode], :].squeeze()\n",
    "            for index, value in pstats.iteritems():\n",
    "                team_flattened_pstats[f\"p{pn}_{index}\"] = value\n",
    "        teams_profiles[tname] = Series(team_flattened_pstats).astype(\"float32\")\n",
    "    return teams_profiles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "\n",
    "# SETUP EVERYTHING \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning dataset temporally (by date):\n",
      "2021-10-28 -> 2022-04-26 : 5385 items in partition\n",
      "2021-05-01 -> 2021-10-28 : 6906 items in partition\n",
      "2020-11-02 -> 2021-05-01 : 5563 items in partition\n",
      "2020-05-06 -> 2020-11-02 : 6222 items in partition\n",
      "2019-11-08 -> 2020-05-06 : 5169 items in partition\n",
      "2019-05-12 -> 2019-11-08 : 6264 items in partition\n",
      "2018-11-13 -> 2019-05-12 : 5846 items in partition\n",
      "2018-05-17 -> 2018-11-13 : 7351 items in partition\n",
      "2017-11-18 -> 2018-05-17 : 6679 items in partition\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import crawler, team_rankings_crawler_and_scraper, matches_stats_scraper\n",
    "\n",
    "\n",
    "def remove_unused_pstats(df : DataFrame, keep_pstat_vars : list [str]) -> DataFrame:\n",
    "    keep_cols = [var for var in list(df.columns) if not((x:=var.split(\"_\")[-1]) in PLAYER_STAT_VARIABLES) or x in keep_pstat_vars] \n",
    "    return df.loc[:, keep_cols ]\n",
    "\n",
    "matches_dataset = remove_unused_pstats(matches_dataset, SELECTED_PSTAT_VARS)\n",
    "matches_dataset = matches_dataset.sort_values(by=[\"match_date\"], ascending=False)\n",
    "matches_dataset\n",
    "\n",
    "dataset_partitions = partition_dataset_by_date(matches_dataset, partition_length=180, overlapping=False)\n",
    "recent_matches_dataset = dataset_partitions.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "\n",
    "# WINNER PREDICTOR MODELS\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_for_classification_models(df : DataFrame, seed : int = 1, test_set_size : float = 0.3):\n",
    "    \n",
    "    teams_profiles : dict[str,Series] = make_teams_profiles(df, pstat_vars=SELECTED_PSTAT_VARS)\n",
    "\n",
    "    # convert all unranked teams to have rank of 31st position\n",
    "    model_dataset = df.copy()\n",
    "    model_dataset[\"t1_rank\"].fillna(31, inplace=True)\n",
    "    model_dataset[\"t2_rank\"].fillna(31, inplace=True)\n",
    "\n",
    "    # extract data for each team, extract player stats later\n",
    "    teams_stats = model_dataset.loc[:,[\"t1_rank\", \"t2_rank\", \"t1_name\", \"t2_name\"]]\n",
    "\n",
    "    # add team player stat profiles to each entry / row of data in features dataframe\n",
    "    features_dict = {}\n",
    "    for index, row in teams_stats.iterrows():\n",
    "        t1_name, t2_name = row[\"t1_name\"], row[\"t2_name\"]\n",
    "        if t1_name in teams_profiles or t2_name in teams_profiles:\n",
    "            if not t1_name in teams_profiles:\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "                t1_profile = Series(list(teams_profiles[t2_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            elif not t2_name in teams_profiles:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t1_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "            else:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            features_dict[index] = pd.concat([row, t1_profile, t2_profile])\n",
    "            # keep_dict[index] = row\n",
    "        # else:\n",
    "            # model_dataset.drop(index, inplace=True)\n",
    "    \n",
    "    features = DataFrame.from_dict(features_dict, orient=\"index\")\n",
    "    features.drop(columns=[\"t1_name\", \"t2_name\"], inplace=True)\n",
    "    features.fillna(0,inplace=True)\n",
    "\n",
    "    # numerical_vars = [var for var in list(features.columns) if var[0] == \"t\" and var[2] == \"p\"]\n",
    "    numerical_vars = list(features.columns)\n",
    "    \n",
    "    labels = model_dataset[\"winner\"].astype(\"category\")\n",
    "    # print(features.shape, labels.shape)\n",
    "    xtrain, xtest, ytrain, ytest =  train_test_split( features, labels, random_state=seed, test_size=test_set_size)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xtrain[numerical_vars])\n",
    "\n",
    "    xtrain[numerical_vars] = scaler.transform(xtrain[numerical_vars]) #scale the training data\n",
    "    xtest[numerical_vars] = scaler.transform(xtest[numerical_vars]) #scale the testing data\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "\n",
    "# todo : add feature importance visualiztions for tuning / selecting features to use in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initial_test_winner_models():\n",
    "    xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(recent_matches_dataset ,seed=69)\n",
    "\n",
    "    basic_rank_pred = []\n",
    "    for index, row in xtest.iterrows():\n",
    "        basic_rank_pred.append(\"t1\" if row[\"t1_rank\"] >= row[\"t2_rank\"] else \"t2\")\n",
    "    print(f'The baseline predictor has a prediction accuracy of {round(accuracy_score(ytest,basic_rank_pred)*100,2)}%')\n",
    "\n",
    "    random.seed(69)\n",
    "    rand_pred = []\n",
    "    for index, row in xtest.iterrows():\n",
    "        rand_pred.append(\"t1\" if random.random() < 0.5 else \"t2\")\n",
    "    print(f'The random predictor has a prediction accuracy of {round(accuracy_score(ytest,rand_pred)*100,2)}%')\n",
    "\n",
    "    ratio_rank_pred = []\n",
    "    for index, row in xtest.iterrows():\n",
    "        ratio_rank_pred.append(\"t1\" if random.random() > float(row[\"t1_rank\"]) / float(row[\"t1_rank\"] + row[\"t2_rank\"]) else \"t2\")\n",
    "    print(f'The ratio rank predictor has a prediction accuracy of {round(accuracy_score(ytest,ratio_rank_pred)*100,2)}%')\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=15)\n",
    "    knn.fit(xtrain,ytrain)\n",
    "    print(f'The KNeighborsClassifier model has a prediction accuracy of {round(knn.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "    dtc = DecisionTreeClassifier(random_state=69)\n",
    "    dtc.fit(xtrain, ytrain)\n",
    "    print(f'The DecisionTreeClassifier model has a prediction accuracy of {round(dtc.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "    rf = RandomForestClassifier(criterion='entropy', n_estimators = 100, random_state = 69)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    print(f'The RandomForestClassifier model has a prediction accuracy of {round(rf.score(xtest, ytest)*100,2)}%')\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(xtrain, ytrain)\n",
    "    print(f'The GaussianNB model has a prediction accuracy of {round(nb.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "    svm = SVC()\n",
    "    svm.fit(xtrain, ytrain)\n",
    "    print(f'The SVC model has a prediction accuracy of {round(svm.score(xtest, ytest)*100,2)}%')\n",
    "    \n",
    "\n",
    "# initial_test_winner_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-28 -> 2022-04-26\n",
      "knn:  n_neighbors : 100\n",
      "dtc:  random_state : 30\n",
      "rf:  n_estimators : 190, random_state : 2\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 0.8, gamma : 0.1\n",
      "2021-05-01 -> 2021-10-28\n",
      "knn:  n_neighbors : 60\n",
      "dtc:  random_state : 1\n",
      "rf:  n_estimators : 190, random_state : 1\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 0.9, gamma : 0.1\n",
      "2020-11-02 -> 2021-05-01\n",
      "knn:  n_neighbors : 60\n",
      "dtc:  random_state : 30\n",
      "rf:  n_estimators : 190, random_state : 1\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 0.8, gamma : 0.1\n",
      "2020-05-06 -> 2020-11-02\n",
      "knn:  n_neighbors : 90\n",
      "dtc:  random_state : 2\n",
      "rf:  n_estimators : 70, random_state : 1\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 0.8, gamma : 0.1\n",
      "2019-11-08 -> 2020-05-06\n",
      "knn:  n_neighbors : 100\n",
      "dtc:  random_state : 6\n",
      "rf:  n_estimators : 190, random_state : 2\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 1.1, gamma : 0.1\n",
      "2019-05-12 -> 2019-11-08\n",
      "knn:  n_neighbors : 100\n",
      "dtc:  random_state : 6\n",
      "rf:  n_estimators : 90, random_state : 260\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 0.9, gamma : 0.1\n",
      "2018-11-13 -> 2019-05-12\n",
      "knn:  n_neighbors : 90\n",
      "dtc:  random_state : 6\n",
      "rf:  n_estimators : 50, random_state : 30\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 1.1, gamma : 0.1\n",
      "2018-05-17 -> 2018-11-13\n",
      "knn:  n_neighbors : 80\n",
      "dtc:  random_state : 260\n",
      "rf:  n_estimators : 50, random_state : 6\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 1.1, gamma : 0.1\n",
      "2017-11-18 -> 2018-05-17\n",
      "knn:  n_neighbors : 20\n",
      "dtc:  random_state : 6\n",
      "rf:  n_estimators : 90, random_state : 1\n",
      "nb:  var_smoothing : 1.0\n",
      "svm:  C : 1.1, gamma : 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############### UNCOMMENT BELOW TO VIEW ALL TUNABLE PARAMS FOR THE VARIOUS MODELS ###############\n",
    "# dummy_knn = KNeighborsClassifier()\n",
    "# print(\"Available params for knn():\")\n",
    "# print(\", \".join([f\"{key}: {dummy_knn.get_params()[key]}\" for key in dummy_knn.get_params()]))\n",
    "\n",
    "# dummy_dtc = DecisionTreeClassifier()\n",
    "# print(\"\\nAvailable params for dtc():\")\n",
    "# print(\", \".join([f\"{key}: {dummy_dtc.get_params()[key]}\" for key in dummy_dtc.get_params()]))\n",
    "\n",
    "# dummy_rf = RandomForestClassifier()\n",
    "# print(\"\\nAvailable params for rf():\")\n",
    "# print(\", \".join([f\"{key}: {dummy_rf.get_params()[key]}\" for key in dummy_rf.get_params()]))\n",
    "\n",
    "# dummy_nb = GaussianNB()\n",
    "# print(\"\\nAvailable params for nb():\")\n",
    "# print(\", \".join([f\"{key}: {dummy_nb.get_params()[key]}\" for key in dummy_nb.get_params()]))\n",
    "\n",
    "# dummy_svm = SVC()\n",
    "# print(\"\\nAvailable params for svc():\")\n",
    "# print(\", \".join([f\"{key}: {dummy_svm.get_params()[key]}\" for key in dummy_svm.get_params()]))\n",
    "\n",
    "\n",
    "def tune_winner_models():\n",
    "\n",
    "    knn_tuning_results = {}\n",
    "    dtc_tuning_results =  {}\n",
    "    rf_tuning_results = {}\n",
    "    nb_tuning_results = {}\n",
    "    svm_tuning_results = {}\n",
    "\n",
    "    for daterange, part in dataset_partitions.iteritems():\n",
    "        print(daterange)\n",
    "        # 10 different seeds for us to average results across\n",
    "        # random_states = [6, 13, 23, 24, 33, 69, 420, 666, 6969, 13666]\n",
    "        # random_states = [69, 420, 666]\n",
    "        random_states = [n + n**n for n in range(0,5)]\n",
    "        # knn_tuning_results_dict =  {}\n",
    "        xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(df=part, seed=1)\n",
    "\n",
    "        result = {}\n",
    "        # xtrain, xtest, ytrain, ytest = convert_dataset_for_models(df=part, seed=seed)\n",
    "        knn_params = {\n",
    "            # \"n_neighbors\": np.arange(1, 100, 10),\n",
    "            \"n_neighbors\": [1] + [n for n in range(10,101,10)],\n",
    "            # \"random_state\" : random_states,\n",
    "        }\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_grid = GridSearchCV(estimator = knn, param_grid = knn_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        knn_grid.fit(xtrain, ytrain)    \n",
    "        knn_tuning_results[daterange] = dict(knn_grid.best_params_) \n",
    "        knn_tuning_results[daterange][\"score\"] = knn_grid.best_score_\n",
    "        print(\"knn: \", \", \".join([f\"{key} : {knn_grid.best_params_[key]}\" for key in knn_grid.best_params_]))\n",
    "\n",
    "\n",
    "        dtc_params = {\n",
    "            # \"n_neighbors\": np.arange(1, 100, 10),\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "        dtc = DecisionTreeClassifier()\n",
    "        dtc_grid = GridSearchCV(estimator = dtc, param_grid = dtc_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        dtc_grid.fit(xtrain, ytrain)    \n",
    "        dtc_tuning_results[daterange] = dict(dtc_grid.best_params_) \n",
    "        dtc_tuning_results[daterange][\"score\"] = dtc_grid.best_score_\n",
    "        print(\"dtc: \", \", \".join([f\"{key} : {dtc_grid.best_params_[key]}\" for key in dtc_grid.best_params_]))\n",
    "\n",
    "        rf_params = {\n",
    "            \"n_estimators\": [1] + [n for n in range(10,201,20)],\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        rf_grid = GridSearchCV(estimator = rf, param_grid = rf_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        rf_grid.fit(xtrain, ytrain)    \n",
    "        rf_tuning_results[daterange] = dict(rf_grid.best_params_) \n",
    "        rf_tuning_results[daterange][\"score\"] = rf_grid.best_score_\n",
    "        print(\"rf: \", \", \".join([f\"{key} : {rf_grid.best_params_[key]}\" for key in rf_grid.best_params_]))\n",
    "\n",
    "\n",
    "        nb_params = {\n",
    "            \"var_smoothing\": np.logspace(0,-9, num=10)\n",
    "        }\n",
    "        nb = GaussianNB()\n",
    "        nb_grid = GridSearchCV(estimator = nb, param_grid = nb_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        nb_grid.fit(xtrain, ytrain)    \n",
    "        nb_tuning_results[daterange] = dict(nb_grid.best_params_) \n",
    "        nb_tuning_results[daterange][\"score\"] = nb_grid.best_score_\n",
    "        print(\"nb: \", \", \".join([f\"{key} : {nb_grid.best_params_[key]}\" for key in nb_grid.best_params_]))\n",
    "\n",
    "        svm_params = {\n",
    "            'C': np.arange(8, 12) * 0.1,\n",
    "            'gamma': np.arange(1, 5) * 0.1,\n",
    "        }\n",
    "        svm = SVC()\n",
    "        svm_grid = GridSearchCV(estimator = svm, param_grid = svm_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        svm_grid.fit(xtrain, ytrain)    \n",
    "        svm_tuning_results[daterange] = dict(svm_grid.best_params_) \n",
    "        svm_tuning_results[daterange][\"score\"] = abs(svm_grid.best_score_)\n",
    "        print(\"svm: \", \", \".join([f\"{key} : {svm_grid.best_params_[key]}\" for key in svm_grid.best_params_]))\n",
    "\n",
    "\n",
    "    return knn_tuning_results, dtc_tuning_results, rf_tuning_results, nb_tuning_results, svm_tuning_results\n",
    "    \n",
    "\n",
    "knn_tuning_results, dtc_tuning_results, rf_tuning_results, nb_tuning_results, svm_tuning_results = tune_winner_models()\n",
    "\n",
    "DataFrame.from_dict(knn_tuning_results, orient=\"index\").to_pickle(\"cache/knn_tuning_results.pkl\")\n",
    "DataFrame.from_dict(dtc_tuning_results, orient=\"index\").to_pickle(\"cache/dtc_tuning_results.pkl\")\n",
    "DataFrame.from_dict(rf_tuning_results, orient=\"index\").to_pickle(\"cache/rf_tuning_results.pkl\")\n",
    "DataFrame.from_dict(nb_tuning_results, orient=\"index\").to_pickle(\"cache/nb_tuning_results.pkl\")\n",
    "DataFrame.from_dict(svm_tuning_results, orient=\"index\").to_pickle(\"cache/svm_tuning_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn_score</th>\n",
       "      <th>dtc_score</th>\n",
       "      <th>rf_score</th>\n",
       "      <th>nb_score</th>\n",
       "      <th>svm_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.632614</td>\n",
       "      <td>0.594361</td>\n",
       "      <td>0.623172</td>\n",
       "      <td>0.616921</td>\n",
       "      <td>0.620508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>0.016191</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>0.014989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.609182</td>\n",
       "      <td>0.573623</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.583207</td>\n",
       "      <td>0.594590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.628665</td>\n",
       "      <td>0.589831</td>\n",
       "      <td>0.612768</td>\n",
       "      <td>0.601957</td>\n",
       "      <td>0.609397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.634208</td>\n",
       "      <td>0.591872</td>\n",
       "      <td>0.616784</td>\n",
       "      <td>0.606949</td>\n",
       "      <td>0.624569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.639838</td>\n",
       "      <td>0.605052</td>\n",
       "      <td>0.634668</td>\n",
       "      <td>0.638503</td>\n",
       "      <td>0.631872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.650539</td>\n",
       "      <td>0.609722</td>\n",
       "      <td>0.647116</td>\n",
       "      <td>0.650297</td>\n",
       "      <td>0.638484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       knn_score  dtc_score  rf_score  nb_score  svm_score\n",
       "count   9.000000   9.000000  9.000000  9.000000   9.000000\n",
       "mean    0.632614   0.594361  0.623172  0.616921   0.620508\n",
       "std     0.012266   0.013257  0.016191  0.024442   0.014989\n",
       "min     0.609182   0.573623  0.597506  0.583207   0.594590\n",
       "25%     0.628665   0.589831  0.612768  0.601957   0.609397\n",
       "50%     0.634208   0.591872  0.616784  0.606949   0.624569\n",
       "75%     0.639838   0.605052  0.634668  0.638503   0.631872\n",
       "max     0.650539   0.609722  0.647116  0.650297   0.638484"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_tuning_results_df : DataFrame = pd.read_pickle(\"cache/knn_tuning_results.pkl\")\n",
    "dtc_tuning_results_df : DataFrame = pd.read_pickle(\"cache/dtc_tuning_results.pkl\")\n",
    "rf_tuning_results_df : DataFrame = pd.read_pickle(\"cache/rf_tuning_results.pkl\")\n",
    "nb_tuning_results_df : DataFrame = pd.read_pickle(\"cache/nb_tuning_results.pkl\")\n",
    "svm_tuning_results_df : DataFrame = pd.read_pickle(\"cache/svm_tuning_results.pkl\")\n",
    "\n",
    "models_best_results = pd.DataFrame(knn_tuning_results_df[\"score\"].rename(\"knn_score\"))\n",
    "models_best_results[\"dtc_score\"] = dtc_tuning_results_df[\"score\"]\n",
    "models_best_results[\"rf_score\"] = rf_tuning_results_df[\"score\"]\n",
    "models_best_results[\"nb_score\"] = nb_tuning_results_df[\"score\"]\n",
    "models_best_results[\"svm_score\"] = svm_tuning_results_df[\"score\"]\n",
    "models_best_results\n",
    "\n",
    "models_best_results_stats = pd.DataFrame(knn_tuning_results_df[\"score\"].rename(\"knn_score\").describe())\n",
    "models_best_results_stats[\"dtc_score\"] = dtc_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats[\"rf_score\"] = rf_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats[\"nb_score\"] = nb_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats[\"svm_score\"] = svm_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16, 9))\n",
    "# sorted_feat = rf.feature_importances_.argsort()\n",
    "# plt.barh(xtrain.columns,rf.feature_importances_[sorted_feat])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "# plot_tree(dtc, \n",
    "#           feature_names=xtrain.columns,\n",
    "#           class_names=[\"t1\", \"t2\"], \n",
    "#           filled=True, impurity=True, \n",
    "#           rounded=True)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "\n",
    "# ROUND WIN DIFF PREDICTOR MODELS\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_for_reg_models(df : DataFrame, seed : int = 1, test_set_size : float = 0.3):\n",
    "    \n",
    "    teams_profiles : dict[str,Series] = make_teams_profiles(df, pstat_vars=SELECTED_PSTAT_VARS)\n",
    "\n",
    "    # convert all unranked teams to have rank of 31st position\n",
    "    model_dataset = df.copy()\n",
    "    model_dataset[\"t1_rank\"].fillna(31, inplace=True)\n",
    "    model_dataset[\"t2_rank\"].fillna(31, inplace=True)\n",
    "\n",
    "    # extract data for each team, extract player stats later\n",
    "    teams_stats = model_dataset.loc[:,[\"winner\",\"t1_rank\", \"t2_rank\", \"t1_name\", \"t2_name\"]]\n",
    "    # keep_dict = {}\n",
    "    # print(teams_stats.shape)\n",
    "    # add team player stat profiles to each entry / row of data in features dataframe\n",
    "    features_dict = {}\n",
    "    for index, row in teams_stats.iterrows():\n",
    "        t1_name, t2_name = row[\"t1_name\"], row[\"t2_name\"]\n",
    "        if t1_name in teams_profiles or t2_name in teams_profiles:\n",
    "            if not t1_name in teams_profiles:\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "                t1_profile = Series(list(teams_profiles[t2_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            elif not t2_name in teams_profiles:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t1_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "            else:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            features_dict[index] = pd.concat([row, t1_profile, t2_profile])\n",
    "            # keep_dict[index] = row\n",
    "        # else:\n",
    "            # model_dataset.drop(index, inplace=True)\n",
    "    \n",
    "    features = DataFrame.from_dict(features_dict, orient=\"index\")\n",
    "    features.drop(columns=[\"t1_name\", \"t2_name\"], inplace=True)\n",
    "    features[\"winner\"] = features[\"winner\"].apply(lambda x: x == \"t1\")\n",
    "    # print(features[\"winner\"])\n",
    "    # print(list(features.columns))\n",
    "\n",
    "    features.fillna(0,inplace=True)\n",
    "\n",
    "    # numerical_vars = [var for var in list(features.columns) if var[0] == \"t\" and var[2] == \"p\"]\n",
    "    numerical_vars = list(features.columns)\n",
    "    \n",
    "    labels = model_dataset[\"rw_diff\"].astype(\"int64\")\n",
    "    # print(features.shape, labels.shape)\n",
    "    xtrain, xtest, ytrain, ytest =  train_test_split( features, labels, random_state=seed, test_size=test_set_size)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xtrain[numerical_vars])\n",
    "\n",
    "    xtrain[numerical_vars] = scaler.transform(xtrain[numerical_vars]) #scale the training data\n",
    "    xtest[numerical_vars] = scaler.transform(xtest[numerical_vars]) #scale the testing data\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(recent_matches_dataset ,seed=69)\n",
    "xtrain.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Regressor Testing/Proof-of-Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_test_rw_diff_models():\n",
    "    xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(dataset_partitions.iloc[1] ,seed=69)\n",
    "    # print(list(xtrain.columns))\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors=15)\n",
    "    knn.fit(xtrain,ytrain)\n",
    "    knn_ypred = knn.predict(xtest)\n",
    "    print(list(knn_ypred)[:10])\n",
    "    print(list(ytest)[:10])\n",
    "    print(f'The KNeighborsRegressor has a RMSE of {round(mean_squared_error(ytest, knn_ypred, squared=False),2)}')\n",
    "    print(f'The KNeighborsRegressor has a MAE of {round(mean_absolute_error(ytest, knn_ypred),2)}')\n",
    "\n",
    "\n",
    "    dtc = DecisionTreeRegressor(random_state=69)\n",
    "    dtc.fit(xtrain, ytrain)\n",
    "    dtc_ypred = dtc.predict(xtest)\n",
    "    print(f'The DecisionTreeRegressor has a RMSE of {round(mean_squared_error(ytest, dtc_ypred, squared=False),2)}')\n",
    "    print(f'The DecisionTreeRegressor has a MAE of {round(mean_absolute_error(ytest, dtc_ypred),2)}')\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(criterion='squared_error', n_estimators = 100, random_state = 69)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    rf_ypred   = rf.predict(xtest)\n",
    "    print(f'The RandomForestRegressor has a RMSE of {round(mean_squared_error(ytest, rf_ypred, squared=False),2)}')\n",
    "    print(f'The RandomForestRegressor has a MAE of {round(mean_absolute_error(ytest, rf_ypred),2)}')\n",
    "\n",
    "    svm = SVR()\n",
    "    svm.fit(xtrain, ytrain)\n",
    "    svm_ypred = svm.predict(xtest)\n",
    "    print(f'The SVR has a RMSE of {round(mean_squared_error(ytest, svm_ypred, squared=False),2)}')\n",
    "    print(f'The SVR has a MAE of {round(mean_absolute_error(ytest, svm_ypred),2)}')\n",
    "\n",
    "initial_test_rw_diff_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_knn = KNeighborsRegressor()\n",
    "print(\"Available params for knn():\")\n",
    "print(\", \".join([f\"{key}: {dummy_knn.get_params()[key]}\" for key in dummy_knn.get_params()]))\n",
    "\n",
    "dummy_dtc = DecisionTreeRegressor()\n",
    "print(\"\\nAvailable params for dtc():\")\n",
    "print(\", \".join([f\"{key}: {dummy_dtc.get_params()[key]}\" for key in dummy_dtc.get_params()]))\n",
    "\n",
    "dummy_rf = RandomForestRegressor()\n",
    "print(\"\\nAvailable params for rf():\")\n",
    "print(\", \".join([f\"{key}: {dummy_rf.get_params()[key]}\" for key in dummy_rf.get_params()]))\n",
    "\n",
    "dummy_svm = SVR()\n",
    "print(\"\\nAvailable params for svr():\")\n",
    "print(\", \".join([f\"{key}: {dummy_svm.get_params()[key]}\" for key in dummy_svm.get_params()]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_rw_diff_models():\n",
    "    knn_reg_tuning_results = {}\n",
    "    dtc_reg_tuning_results =  {}\n",
    "    rf_reg_tuning_results = {}\n",
    "    svm_reg_tuning_results = {}\n",
    "\n",
    "    for daterange, part in dataset_partitions.iteritems():\n",
    "        print(daterange)\n",
    "        # 10 different seeds for us to average results across\n",
    "        # random_states = [6, 13, 23, 24, 33, 69, 420, 666, 6969, 13666]\n",
    "        # random_states = [69, 420, 666]\n",
    "        random_states = [n + n**n for n in range(0,5)]\n",
    "        # knn_tuning_results_dict =  {}\n",
    "        xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(df=part, seed=1)\n",
    "\n",
    "        # xtrain, xtest, ytrain, ytest = convert_dataset_for_models(df=part, seed=seed)\n",
    "        knn_params = {\n",
    "            # \"n_neighbors\": np.arange(1, 100, 10),\n",
    "            \"n_neighbors\": [1] + [n for n in range(10,101,10)],\n",
    "            # \"random_state\" : random_states,\n",
    "        }\n",
    "        knn = KNeighborsRegressor()        \n",
    "        knn_grid = GridSearchCV(estimator = knn, param_grid = knn_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        knn_grid.fit(xtrain, ytrain)    \n",
    "        knn_reg_tuning_results[daterange] = dict(knn_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        knn_reg_tuning_results[daterange][\"score\"] = abs(knn_grid.best_score_)\n",
    "        \n",
    "        print(\"knn: \", \", \".join([f\"{key} : {knn_grid.best_params_[key]}\" for key in knn_grid.best_params_]))\n",
    "\n",
    "\n",
    "        dtc_params = {\n",
    "            # \"n_neighbors\": np.arange(1, 100, 10),\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "        dtc = DecisionTreeRegressor()\n",
    "        dtc_grid = GridSearchCV(estimator = dtc, param_grid = dtc_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        dtc_grid.fit(xtrain, ytrain)    \n",
    "        dtc_reg_tuning_results[daterange] = dict(dtc_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        dtc_reg_tuning_results[daterange][\"score\"] = abs(dtc_grid.best_score_)\n",
    "\n",
    "        print(\"dtc: \", \", \".join([f\"{key} : {dtc_grid.best_params_[key]}\" for key in dtc_grid.best_params_]))\n",
    "\n",
    "        rf_params = {\n",
    "            \"n_estimators\": [1] + [n for n in range(20,201,20)],\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "        rf = RandomForestRegressor()\n",
    "        rf_grid = GridSearchCV(estimator = rf, param_grid = rf_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        rf_grid.fit(xtrain, ytrain)    \n",
    "        rf_reg_tuning_results[daterange] = dict(rf_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        rf_reg_tuning_results[daterange][\"score\"] = abs(rf_grid.best_score_)\n",
    "\n",
    "        print(\"rf: \", \", \".join([f\"{key} : {rf_grid.best_params_[key]}\" for key in rf_grid.best_params_]))\n",
    "            \n",
    "        svm_params = {\n",
    "            'C': np.arange(8, 12) * 0.1,\n",
    "            'gamma': np.arange(1, 5) * 0.1,\n",
    "        }\n",
    "        svm = SVR()\n",
    "        svm_grid = GridSearchCV(estimator = svm, param_grid = svm_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        svm_grid.fit(xtrain, ytrain)    \n",
    "        svm_reg_tuning_results[daterange] = dict(svm_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        svm_reg_tuning_results[daterange][\"score\"] = abs(svm_grid.best_score_)\n",
    "        print(\"svm: \", \", \".join([f\"{key} : {svm_grid.best_params_[key]}\" for key in svm_grid.best_params_]))\n",
    "\n",
    "\n",
    "    return knn_reg_tuning_results, dtc_reg_tuning_results, rf_reg_tuning_results, svm_reg_tuning_results\n",
    "knn_reg_tuning_results, dtc_reg_tuning_results, rf_reg_tuning_results, svm_reg_tuning_results = tune_rw_diff_models()\n",
    "\n",
    "DataFrame.from_dict(knn_reg_tuning_results, orient=\"index\").to_pickle(\"cache/knn_reg_tuning_results.pkl\")\n",
    "DataFrame.from_dict(dtc_reg_tuning_results, orient=\"index\").to_pickle(\"cache/dtc_reg_tuning_results.pkl\")\n",
    "DataFrame.from_dict(rf_reg_tuning_results, orient=\"index\").to_pickle(\"cache/rf_reg_tuning_results.pkl\")\n",
    "DataFrame.from_dict(svm_reg_tuning_results, orient=\"index\").to_pickle(\"cache/svm_reg_tuning_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# knn_reg_tuning_results_df = pd.read_pickle(\"cache/knn_reg_tuning_results.pkl\")\n",
    "# dtc_reg_tuning_results_df = DataFrame.from_dict(dtc_reg_tuning_results, orient=\"index\")\n",
    "# rf_reg_tuning_results_df = DataFrame.from_dict(rf_reg_tuning_results, orient=\"index\")\n",
    "svm_reg_tuning_results_df = pd.read_pickle(\"cache/svm_reg_tuning_results.pkl\")\n",
    "\n",
    "# print(\"KNN Regression Tuning Results:\")\n",
    "# print(knn_reg_tuning_results_df)\n",
    "# print(\"DTC Regression Tuning Results:\")\n",
    "# print(dtc_reg_tuning_results_df)\n",
    "# print(\"RF Regression Tuning Results:\")\n",
    "# print(rf_reg_tuning_results_df)\n",
    "print(\"SVM Regression Tuning Results:\")\n",
    "print(svm_reg_tuning_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = [n + n**n for n in range(0,10)]\n",
    "random_states"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
