{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- \n",
    "\n",
    "# **Table of Contents:**\n",
    "\n",
    "* [Setup And Initialization](#setup-and-initialization)\n",
    "  * [x](#part)\n",
    "* [Python Web Scrapers](#python-web-scrapers) \n",
    "  * [HLTV Matches URL Scraper](#hltv-matches-url-scraper)\n",
    "  * [HLTV Team Rankings Scraper](#hltv-team-rankings-scraper)\n",
    "  * [HLTV Matches Stats Scraper](#hltv-matches-stats-scraper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "# **Pre-Work And Setup Code**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import All Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set() \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "\n",
    "# **EDA and Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load the Matches Stats CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.read_csv(\"https://raw.githubusercontent.com/a-dubs/ds3000-project/master/all_matches_stats.csv\")\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load the Team Rankings CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_df = pd.read_csv(\"https://raw.githubusercontent.com/a-dubs/ds3000-project/master/team_rankings.csv\")\n",
    "rankings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Converting and Cleaning Raw Scraped Data to Usable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now lets add ranking team ranking information to the matches dataset #\n",
    "# extract the two team name columns\n",
    "t1_names = matches_df[\"t1_name\"]\n",
    "t2_names = matches_df[\"t2_name\"]\n",
    "\n",
    "t1_rankings = []\n",
    "t2_rankings = []\n",
    "# with alive_bar(len(matches_df), force_tty=True):\n",
    "for i in range(len(matches_df)):\n",
    "    date = matches_df[\"match_date\"].iloc[i]\n",
    "    rankings_series = rankings_df.loc[rankings_df[\"date\"] == date, :].squeeze()\n",
    "    t1_rank = int(x.index[0].split(\"_\")[0]) if (x:=rankings_series[rankings_series == t1_names[i]]).size == 1 else 0\n",
    "    t2_rank = int(x.index[0].split(\"_\")[0]) if (x:=rankings_series[rankings_series == t2_names[i]]).size == 1 else 0\n",
    "    t1_rankings.append(t1_rank)\n",
    "    t2_rankings.append(t2_rank)\n",
    "        \n",
    "t1_rankings = pd.Series(t1_rankings).replace(0, np.nan)\n",
    "t2_rankings = pd.Series(t2_rankings).replace(0, np.nan)\n",
    "\n",
    "matches_dataset = matches_df.copy()\n",
    "\n",
    "winners = []\n",
    "for index, row in matches_dataset.iterrows():\n",
    "    winners.append(\"t1\" if row['t1_total_rw'] > row['t2_total_rw'] else \"t2\")\n",
    "\n",
    "map_col_index = matches_dataset.columns.get_loc(\"map\")\n",
    "matches_dataset.insert(map_col_index, \"winner\", winners, allow_duplicates=False)\n",
    "\n",
    "round_win_diffs = []\n",
    "for index, row in matches_dataset.iterrows():\n",
    "    round_win_diffs.append(row['t1_total_rw'] - row['t2_total_rw'])\n",
    "\n",
    "winner_col_index = matches_dataset.columns.get_loc(\"winner\")\n",
    "matches_dataset.insert(winner_col_index + 1, \"rw_diff\", round_win_diffs, allow_duplicates=False)\n",
    "\n",
    "t1_rank_col_index = matches_dataset.columns.get_loc(\"t1_id\") \n",
    "matches_dataset.insert(t1_rank_col_index + 1, \"t1_rank\", t1_rankings, allow_duplicates=False)\n",
    "# get updated index now that column has been inserted prior\n",
    "t2_rank_col_index = matches_dataset.columns.get_loc(\"t2_id\") \n",
    "matches_dataset.insert(t2_rank_col_index + 1, \"t2_rank\", t2_rankings, allow_duplicates=False)\n",
    "\n",
    "# unranked teams will be given rank of 31 (one lower than the lowest rank possible)\n",
    "matches_dataset[\"t1_rank\"].fillna(31, inplace=True)\n",
    "matches_dataset[\"t2_rank\"].fillna(31, inplace=True)\n",
    "# replace stray '-' with NaN\n",
    "matches_dataset.replace(\"-\",None, inplace=True)\n",
    "\n",
    "# convert kdratio variables to decimal and make sure rating and adr columns are floats\n",
    "for tn in (1,2):\n",
    "    matches_dataset[f\"t{tn}_rating\"] = matches_dataset[f\"t{tn}_rating\"].astype(\"float32\")\n",
    "    for pn in (1,2,3,4,5):\n",
    "        matches_dataset[f\"t{tn}p{pn}_kdratio\"].fillna(\"50%\",inplace=True)\n",
    "        try:\n",
    "            matches_dataset[f\"t{tn}p{pn}_kdratio\"] = matches_dataset[f\"t{tn}p{pn}_kdratio\"].apply(\n",
    "                lambda x: 0.01*float(x.replace(\"%\",\"\")) if x else None)\n",
    "        except:\n",
    "            print(\":(\")\n",
    "            \n",
    "        matches_dataset[f\"t{tn}p{pn}_adr\"] = matches_dataset[f\"t{tn}p{pn}_adr\"].astype(\"float32\")\n",
    "        matches_dataset[f\"t{tn}p{pn}_rating\"] = matches_dataset[f\"t{tn}p{pn}_rating\"].astype(\"float32\")\n",
    "datatypes = list(matches_dataset.dtypes)\n",
    "columns = list(matches_dataset.columns)\n",
    "# print({columns[i]:datatypes[i] for i in range(len(columns))})\n",
    "\n",
    "# force team names to be lowercase\n",
    "matches_dataset[\"t1_name\"] = matches_dataset[\"t1_name\"].str.lower()\n",
    "matches_dataset[\"t2_name\"] = matches_dataset[\"t2_name\"].str.lower()\n",
    "\n",
    "# apply categorical data type \n",
    "matches_dataset[\"map\"] = matches_dataset[\"map\"].astype(\"category\") \n",
    "matches_dataset[\"t1_name\"] = matches_dataset[\"t1_name\"].astype(\"category\")\n",
    "matches_dataset[\"t2_name\"] = matches_dataset[\"t2_name\"].astype(\"category\")  \n",
    "\n",
    "matches_dataset.to_csv(index=False, path_or_buf=\"all_matches_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Loading the Newly Prepared and Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All player stats variables: kills, hskills, assists, fassists, deaths, kdratio, kddiff, adr, fkdiff, rating\n",
      "SELECTED player stats variables: kills, kdratio, rating\n"
     ]
    }
   ],
   "source": [
    "matches_dataset = pd.read_csv(\"all_matches_dataset.csv\")\n",
    "\n",
    "PLAYER_STAT_VARIABLES = [var.replace(\"t1p1_\",\"\") for var in list(matches_dataset.columns) if (\"t1p1_\" in var) and (not \"player\" in var)]\n",
    "print(\"All player stats variables:\", \", \".join(PLAYER_STAT_VARIABLES))\n",
    "\n",
    "many_pstat_vars = [\"kills\", \"assists\", \"deaths\", \"kdratio\", \"kddiff\", \"adr\", \"fkdiff\", \"rating\"]\n",
    "few_pstat_vars = [\"kills\", \"kdratio\", \"kddiff\", \"fkdiff\", \"rating\"]\n",
    "min_pstat_vars = [\"kills\", \"kdratio\", \"rating\"]\n",
    "\n",
    "SELECTED_PSTAT_VARS = min_pstat_vars\n",
    "print(\"SELECTED player stats variables:\", \", \".join(SELECTED_PSTAT_VARS))\n",
    "\n",
    "matches_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "def filter_dataset_by_date(full_df, start_dt : datetime, stop_dt : datetime, date_col_name : str = \"match_date\"):\n",
    "    df : DataFrame = full_df.copy()\n",
    "    keep_dict = {}\n",
    "    if start_dt > stop_dt:\n",
    "        print(\"datetimes were given backwards\")\n",
    "        temp_dt = start_dt\n",
    "        start_dt = stop_dt\n",
    "        stop_dt = temp_dt\n",
    "    for index, row in df.iterrows():\n",
    "        if not((dt:=datetime.strptime(row[date_col_name], '%Y-%m-%d')) < start_dt or dt >= stop_dt):\n",
    "            # keep_indeces.append(index)\n",
    "            # keep_rows.append(row)\n",
    "            keep_dict[index] = row \n",
    "    return DataFrame.from_dict(keep_dict,orient=\"index\")\n",
    "\n",
    "#######################################################################################################################\n",
    "def partition_dataset_by_date(df : DataFrame, stop_date = \"2017-08-01\", partition_length : int = 120, overlapping : bool = True, partition_shift : int = 30, pstat_vars_filter = None, dt_it_init_val = None) -> Series:\n",
    "    partitions = {}\n",
    "    stop_dt = datetime.strptime(stop_date, '%Y-%m-%d')\n",
    "    dt_it = dt_it_init_val if dt_it_init_val else datetime.now()\n",
    "    if not overlapping:\n",
    "        partition_shift = partition_length\n",
    "    print(\"Partitioning dataset temporally (by date):\")\n",
    "    while dt_it - timedelta(days=(partition_length)) > stop_dt:\n",
    "        partition = filter_dataset_by_date(df, date_col_name= \"match_date\", start_dt=dt_it - timedelta(days = partition_length), stop_dt=dt_it)\n",
    "        date_range = f\"{datetime.strftime(dt_it - timedelta(days = partition_length),'%Y-%m-%d')} -> {datetime.strftime(dt_it,'%Y-%m-%d')}\"\n",
    "        partitions[date_range] = partition\n",
    "        print(date_range,\":\",partition.shape[0],\"items in partition\")\n",
    "        dt_it = dt_it - timedelta(days=partition_shift)\n",
    "    return Series(partitions)\n",
    "\n",
    "#######################################################################################################################\n",
    "def get_aggregate_players_stats(df : DataFrame, pstat_vars = SELECTED_PSTAT_VARS):\n",
    "    teams_pstats = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for tn in (1,2):\n",
    "            # score_diff = row[\"t1_total_rw\"] - row[\"t2_total_rw\"] if tn==1 else row[\"t2_total_rw\"] - row[\"t1_total_rw\"]\n",
    "            tname = row[f\"t{tn}_name\"]\n",
    "            \n",
    "            for pn in (1,2,3,4,5):\n",
    "                id = f\"t{tn}p{pn}_\"\n",
    "                pname = row[id+\"player\"]\n",
    "                if teams_pstats.get(tname) == None:\n",
    "                    teams_pstats[tname] = {}\n",
    "                if teams_pstats[tname].get(pname) == None:\n",
    "                    teams_pstats[tname][pname] = []\n",
    "                teams_pstats[tname][pname].append({var:row[id+var] for var in pstat_vars})\n",
    "    return teams_pstats\n",
    "\n",
    "#######################################################################################################################\n",
    "def get_active_rosters(matches_df) -> dict[str, list[str]]:\n",
    "    aggregate_pstats = get_aggregate_players_stats(matches_df)\n",
    "    teams_active_players = {}\n",
    "    for tname in aggregate_pstats:\n",
    "        team = aggregate_pstats[tname]\n",
    "        players_games_played = {}\n",
    "        # players_games_played =  {player: for player in team}\n",
    "        for pname in team:\n",
    "            player = team[pname]\n",
    "            players_games_played[pname] = len(list(player))\n",
    "        active_players = list(pd.Series(players_games_played).sort_values(ascending=False).iloc[:5].index)\n",
    "        teams_active_players[tname] = active_players\n",
    "    return teams_active_players\n",
    "\n",
    "#######################################################################################################################\n",
    "def get_sorted_teams_rosters(df : DataFrame, rosters, mode = \"mean\"):\n",
    "    aggregated_teams_pstats = get_aggregate_players_stats(df)\n",
    "    sorted_rosters = {}\n",
    "    for tname in rosters:\n",
    "        team = aggregated_teams_pstats[tname]\n",
    "        pn = 0\n",
    "        pratings = {}\n",
    "        for pname in rosters[tname]:\n",
    "            pmatches = pd.DataFrame(team[pname])\n",
    "            prating_stats = pmatches.loc[:,[\"rating\"]].describe().squeeze()\n",
    "            prating_agg_val = prating_stats[mode]\n",
    "            pratings[pname] = prating_agg_val\n",
    "        \n",
    "        sorted_rosters[tname] = list(Series(pratings).sort_values(ascending=False).index)\n",
    "    return sorted_rosters\n",
    "\n",
    "#######################################################################################################################\n",
    "def make_teams_profiles(df : DataFrame, pstat_vars = SELECTED_PSTAT_VARS, mode = \"mean\"):\n",
    "    \n",
    "    rosters = get_sorted_teams_rosters(df, rosters=get_active_rosters(df))\n",
    "\n",
    "    aggregated_teams_pstats = get_aggregate_players_stats(df, pstat_vars=pstat_vars)\n",
    "\n",
    "    teams_profiles = {}\n",
    "    for tname in rosters:\n",
    "        team = aggregated_teams_pstats[tname]\n",
    "        team_flattened_pstats = {}\n",
    "        pn = 0\n",
    "        for pname in rosters[tname]:\n",
    "            pn += 1\n",
    "            pmatches = pd.DataFrame(team[pname])\n",
    "            pmatches_stats = pmatches.describe()\n",
    "            pstats : Series = pmatches_stats.loc[[mode], :].squeeze()\n",
    "            for index, value in pstats.iteritems():\n",
    "                team_flattened_pstats[f\"p{pn}_{index}\"] = value\n",
    "        teams_profiles[tname] = Series(team_flattened_pstats).astype(\"float32\")\n",
    "    return teams_profiles\n",
    "\n",
    "#######################################################################################################################\n",
    "def remove_unused_pstats(df : DataFrame, keep_pstat_vars : list [str]) -> DataFrame:\n",
    "    keep_cols = [var for var in list(df.columns) if not((x:=var.split(\"_\")[-1]) in PLAYER_STAT_VARIABLES) or x in keep_pstat_vars] \n",
    "    return df.loc[:, keep_cols ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Partitioning and Final Prep of the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning dataset temporally (by date):\n",
      "2021-10-28 -> 2022-04-26 : 5385 items in partition\n",
      "2021-05-01 -> 2021-10-28 : 6906 items in partition\n",
      "2020-11-02 -> 2021-05-01 : 5563 items in partition\n",
      "2020-05-06 -> 2020-11-02 : 6222 items in partition\n",
      "2019-11-08 -> 2020-05-06 : 5169 items in partition\n",
      "2019-05-12 -> 2019-11-08 : 6264 items in partition\n",
      "2018-11-13 -> 2019-05-12 : 5846 items in partition\n",
      "2018-05-17 -> 2018-11-13 : 7351 items in partition\n",
      "2017-11-18 -> 2018-05-17 : 6679 items in partition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_url</th>\n",
       "      <th>match_date</th>\n",
       "      <th>winner</th>\n",
       "      <th>rw_diff</th>\n",
       "      <th>map</th>\n",
       "      <th>t1_name</th>\n",
       "      <th>t1_id</th>\n",
       "      <th>t1_rank</th>\n",
       "      <th>t1_total_rw</th>\n",
       "      <th>t1_fh_rw</th>\n",
       "      <th>...</th>\n",
       "      <th>t2p3_kdratio</th>\n",
       "      <th>t2p3_rating</th>\n",
       "      <th>t2p4_player</th>\n",
       "      <th>t2p4_kills</th>\n",
       "      <th>t2p4_kdratio</th>\n",
       "      <th>t2p4_rating</th>\n",
       "      <th>t2p5_player</th>\n",
       "      <th>t2p5_kills</th>\n",
       "      <th>t2p5_kdratio</th>\n",
       "      <th>t2p5_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/stats/matches/mapstatsid/46584/hellraisers-vs...</td>\n",
       "      <td>2017-05-17</td>\n",
       "      <td>t1</td>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "      <td>hellraisers</td>\n",
       "      <td>5310</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.40</td>\n",
       "      <td>LEGIJA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.35</td>\n",
       "      <td>keev</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/stats/matches/mapstatsid/62534/envy-vs-north</td>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>t1</td>\n",
       "      <td>2</td>\n",
       "      <td>cobblestone</td>\n",
       "      <td>north</td>\n",
       "      <td>7533</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.17</td>\n",
       "      <td>Happy</td>\n",
       "      <td>21</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.15</td>\n",
       "      <td>xms</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/stats/matches/mapstatsid/61361/fnatic-vs-gambit</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>t1</td>\n",
       "      <td>8</td>\n",
       "      <td>inferno</td>\n",
       "      <td>fnatic</td>\n",
       "      <td>4991</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.76</td>\n",
       "      <td>mou</td>\n",
       "      <td>12</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Dosia</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/stats/matches/mapstatsid/57159/liquid-vs-cloud9</td>\n",
       "      <td>2017-11-12</td>\n",
       "      <td>t2</td>\n",
       "      <td>-11</td>\n",
       "      <td>overpass</td>\n",
       "      <td>liquid</td>\n",
       "      <td>5973</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857</td>\n",
       "      <td>1.39</td>\n",
       "      <td>Skadoodle</td>\n",
       "      <td>10</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.10</td>\n",
       "      <td>RUSH</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/stats/matches/mapstatsid/123194/dbl-poney-vs-ago</td>\n",
       "      <td>2021-07-04</td>\n",
       "      <td>t1</td>\n",
       "      <td>2</td>\n",
       "      <td>ancient</td>\n",
       "      <td>dbl-poney</td>\n",
       "      <td>11003</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.09</td>\n",
       "      <td>reatz</td>\n",
       "      <td>17</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.89</td>\n",
       "      <td>rallen</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62985</th>\n",
       "      <td>/stats/matches/mapstatsid/128100/hreds-vs-aab</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>t2</td>\n",
       "      <td>-7</td>\n",
       "      <td>overpass</td>\n",
       "      <td>hreds</td>\n",
       "      <td>10116</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.93</td>\n",
       "      <td>Buzz</td>\n",
       "      <td>12</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.80</td>\n",
       "      <td>jarko</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62986</th>\n",
       "      <td>/stats/matches/mapstatsid/96383/exploit-vs-offset</td>\n",
       "      <td>2019-12-08</td>\n",
       "      <td>t1</td>\n",
       "      <td>8</td>\n",
       "      <td>dust2</td>\n",
       "      <td>exploit</td>\n",
       "      <td>10192</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.78</td>\n",
       "      <td>pr</td>\n",
       "      <td>11</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.72</td>\n",
       "      <td>obj</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62987</th>\n",
       "      <td>/stats/matches/mapstatsid/106567/syman-vs-tiki...</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>t2</td>\n",
       "      <td>-8</td>\n",
       "      <td>mirage</td>\n",
       "      <td>syman</td>\n",
       "      <td>8772</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.32</td>\n",
       "      <td>shushan</td>\n",
       "      <td>17</td>\n",
       "      <td>0.792</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Djury</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62988</th>\n",
       "      <td>/stats/matches/mapstatsid/72336/nordavind-vs-3...</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>t1</td>\n",
       "      <td>4</td>\n",
       "      <td>inferno</td>\n",
       "      <td>3dmax</td>\n",
       "      <td>4914</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.81</td>\n",
       "      <td>ztk</td>\n",
       "      <td>17</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.79</td>\n",
       "      <td>becker</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62989</th>\n",
       "      <td>/stats/matches/mapstatsid/55000/scarz-absolute...</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>t2</td>\n",
       "      <td>-11</td>\n",
       "      <td>inferno</td>\n",
       "      <td>scarz-absolute</td>\n",
       "      <td>7962</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.29</td>\n",
       "      <td>PTC</td>\n",
       "      <td>14</td>\n",
       "      <td>0.905</td>\n",
       "      <td>1.22</td>\n",
       "      <td>QKA</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.762</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62990 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               match_url  match_date winner  \\\n",
       "0      /stats/matches/mapstatsid/46584/hellraisers-vs...  2017-05-17     t1   \n",
       "1          /stats/matches/mapstatsid/62534/envy-vs-north  2018-03-08     t1   \n",
       "2       /stats/matches/mapstatsid/61361/fnatic-vs-gambit  2018-02-17     t1   \n",
       "3       /stats/matches/mapstatsid/57159/liquid-vs-cloud9  2017-11-12     t2   \n",
       "4      /stats/matches/mapstatsid/123194/dbl-poney-vs-ago  2021-07-04     t1   \n",
       "...                                                  ...         ...    ...   \n",
       "62985      /stats/matches/mapstatsid/128100/hreds-vs-aab  2021-10-05     t2   \n",
       "62986  /stats/matches/mapstatsid/96383/exploit-vs-offset  2019-12-08     t1   \n",
       "62987  /stats/matches/mapstatsid/106567/syman-vs-tiki...  2020-08-01     t2   \n",
       "62988  /stats/matches/mapstatsid/72336/nordavind-vs-3...  2018-08-22     t1   \n",
       "62989  /stats/matches/mapstatsid/55000/scarz-absolute...  2017-10-14     t2   \n",
       "\n",
       "       rw_diff          map         t1_name  t1_id  t1_rank  t1_total_rw  \\\n",
       "0           14        train     hellraisers   5310      8.0           16   \n",
       "1            2  cobblestone           north   7533     13.0           16   \n",
       "2            8      inferno          fnatic   4991      5.0           16   \n",
       "3          -11     overpass          liquid   5973      8.0            5   \n",
       "4            2      ancient       dbl-poney  11003     25.0           16   \n",
       "...        ...          ...             ...    ...      ...          ...   \n",
       "62985       -7     overpass           hreds  10116     31.0            9   \n",
       "62986        8        dust2         exploit  10192     31.0           16   \n",
       "62987       -8       mirage           syman   8772     31.0            8   \n",
       "62988        4      inferno           3dmax   4914     31.0           16   \n",
       "62989      -11      inferno  scarz-absolute   7962     31.0            5   \n",
       "\n",
       "       t1_fh_rw  ...  t2p3_kdratio  t2p3_rating  t2p4_player  t2p4_kills  \\\n",
       "0            13  ...         0.500         0.40       LEGIJA           5   \n",
       "1             8  ...         0.800         1.17        Happy          21   \n",
       "2            10  ...         0.583         0.76          mou          12   \n",
       "3             3  ...         0.857         1.39    Skadoodle          10   \n",
       "4             6  ...         0.800         1.09        reatz          17   \n",
       "...         ...  ...           ...          ...          ...         ...   \n",
       "62985         7  ...         0.640         0.93         Buzz          12   \n",
       "62986         8  ...         0.500         0.78           pr          11   \n",
       "62987         1  ...         0.667         1.32      shushan          17   \n",
       "62988         7  ...         0.571         0.81          ztk          17   \n",
       "62989         4  ...         0.810         1.29          PTC          14   \n",
       "\n",
       "       t2p4_kdratio t2p4_rating  t2p5_player  t2p5_kills  t2p5_kdratio  \\\n",
       "0             0.333        0.35         keev         5.0         0.333   \n",
       "1             0.700        1.15          xms        15.0         0.633   \n",
       "2             0.542        0.75        Dosia         7.0         0.458   \n",
       "3             0.810        1.10         RUSH        16.0         0.714   \n",
       "4             0.700        0.89       rallen        15.0         0.567   \n",
       "...             ...         ...          ...         ...           ...   \n",
       "62985         0.640        0.80        jarko        13.0         0.560   \n",
       "62986         0.667        0.72          obj         7.0         0.542   \n",
       "62987         0.792        1.20        Djury        10.0         0.708   \n",
       "62988         0.536        0.79       becker        14.0         0.571   \n",
       "62989         0.905        1.22          QKA        14.0         0.762   \n",
       "\n",
       "      t2p5_rating  \n",
       "0            0.26  \n",
       "1            0.78  \n",
       "2            0.37  \n",
       "3            1.00  \n",
       "4            0.81  \n",
       "...           ...  \n",
       "62985        0.80  \n",
       "62986        0.51  \n",
       "62987        0.82  \n",
       "62988        0.69  \n",
       "62989        1.18  \n",
       "\n",
       "[62990 rows x 65 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_dataset = remove_unused_pstats(matches_dataset, SELECTED_PSTAT_VARS)\n",
    "matches_dataset = matches_dataset.sort_values(by=[\"match_date\"], ascending=False)\n",
    "\n",
    "matches_dataset[\"t1_rank\"].fillna(31, inplace=True)\n",
    "matches_dataset[\"t2_rank\"].fillna(31, inplace=True)\n",
    "\n",
    "matches_dataset = matches_dataset.sort_index()\n",
    "\n",
    "dataset_partitions = partition_dataset_by_date(matches_dataset, partition_length=180, overlapping=False, dt_it_init_val=datetime(year=2022,month=4,day=26,hour=12))\n",
    "recent_matches_dataset = dataset_partitions.iloc[0]\n",
    "\n",
    "matches_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Visualizing Upsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsets !\n",
    "upsets_df = matches_dataset.copy()\n",
    "upsets_df = upsets_df.loc[(upsets_df[\"t1_rank\"] != upsets_df[\"t2_rank\"]),:]\n",
    "\n",
    "# print(upsets_df.loc[:,[\"t1_rank\", \"t2_rank\", \"rw_diff\", \"t1_total_rw\", \"t2_total_rw\"]].head(10))\n",
    "# higher_rank_team = []\n",
    "\n",
    "# for index, row in eda_df.iterrows():\n",
    "#     rank_diff = row[\"t1_rank\"] - [\"t2_rank\"]\n",
    "#     rw_diff = row[\"t1_rw_diff\"] - [\"t2_rw_diff\"]\n",
    "\n",
    "# eda_df[\"winner\"].value_counts().plot.pie()\n",
    "\n",
    "\n",
    "# eda_df.loc[:, [\"winner\", \"rw_diff\"]].value_counts(\"abs\").plot()\n",
    "# rank_diffs = []\n",
    "# rw_diffs = []\n",
    "# for index, row in eda_df.iterrows():\n",
    "#     rank_diff = row[\"t1_rank\"] - [\"t2_rank\"]\n",
    "#     rw_diff = row[\"t1_rw_diff\"] - [\"t2_rw_diff\"]\n",
    "\n",
    "# pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# pie chart of # of games won by higher rank teams vs # of games won by lower rank team\n",
    "upset_or_not = {}\n",
    "\n",
    "for index, row in upsets_df.iterrows():\n",
    "    upset_or_not[index] = \"not upset\" if (((row[\"t1_rank\"] - row[\"t2_rank\"]) * row[\"rw_diff\"]) < 0) else \"upset\"\n",
    "upset_or_not = Series(upset_or_not)\n",
    "# print(upset_or_not.head(10))\n",
    "\n",
    "upset_frequencies = upset_or_not.value_counts(normalize=True)\n",
    "plt.pie(upset_frequencies)\n",
    "# line graph of x var: abs(rank disparity), y var: % of games won by the higher rank team \n",
    "# grouped bar chart for aggregate player ratings of winning teams vs losing teams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsets_per_rank_diff = Series({n:0 for n in range(1,31)}, name=\"upsets_freq\")\n",
    "non_upsets_per_rank_diff = Series({n:0 for n in range(1,31)}, name=\"non_upsets_freq\")\n",
    "for index, row in upsets_df.iterrows():\n",
    "    if upset_or_not[index] == \"upset\":\n",
    "        upsets_per_rank_diff[abs(row[\"t1_rank\"] - row[\"t2_rank\"])] += 1\n",
    "    else:\n",
    "        non_upsets_per_rank_diff[abs(row[\"t1_rank\"] - row[\"t2_rank\"])] += 1\n",
    "\n",
    "\n",
    "upsets_or_not_vs_rank_diffs = DataFrame(upsets_per_rank_diff)\n",
    "upsets_or_not_vs_rank_diffs[\"non_upsets_freq\"] = non_upsets_per_rank_diff\n",
    "\n",
    "upsets_rel_freq_vs_rank_diffs = Series({n:0 for n in range(1,31)})\n",
    "for index, row in upsets_or_not_vs_rank_diffs.iterrows():\n",
    "    upsets_rel_freq_vs_rank_diffs[index] = round(row[\"upsets_freq\"] / (row[\"upsets_freq\"] + row[\"non_upsets_freq\"]) * 100, 2)\n",
    "plt.ylabel(\"Percent of Matches That Are Upsets\")\n",
    "plt.xlabel(\"Rank Disparity Between Teams\")\n",
    "plt.plot(upsets_rel_freq_vs_rank_diffs.index, upsets_rel_freq_vs_rank_diffs.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "\n",
    "# **Match Winner Predictor Classification Models**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Converting Dataset for The Classification Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_for_classification_models(df : DataFrame, seed : int = 1, test_set_size : float = 0.3):\n",
    "    \n",
    "    teams_profiles : dict[str,Series] = make_teams_profiles(df, pstat_vars=SELECTED_PSTAT_VARS)\n",
    "\n",
    "    # convert all unranked teams to have rank of 31st position\n",
    "    model_dataset = df.copy()\n",
    "    model_dataset[\"t1_rank\"].fillna(31, inplace=True)\n",
    "    model_dataset[\"t2_rank\"].fillna(31, inplace=True)\n",
    "\n",
    "    # extract data for each team, extract player stats later\n",
    "    teams_stats = model_dataset.loc[:,[\"t1_rank\", \"t2_rank\", \"t1_name\", \"t2_name\"]]\n",
    "\n",
    "    # add team player stat profiles to each entry / row of data in features dataframe\n",
    "    features_dict = {}\n",
    "    for index, row in teams_stats.iterrows():\n",
    "        t1_name, t2_name = row[\"t1_name\"], row[\"t2_name\"]\n",
    "        if t1_name in teams_profiles or t2_name in teams_profiles:\n",
    "            if not t1_name in teams_profiles:\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "                t1_profile = Series(list(teams_profiles[t2_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            elif not t2_name in teams_profiles:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t1_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "            else:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            features_dict[index] = pd.concat([row, t1_profile, t2_profile])\n",
    "            # keep_dict[index] = row\n",
    "        # else:\n",
    "            # model_dataset.drop(index, inplace=True)\n",
    "    \n",
    "    features = DataFrame.from_dict(features_dict, orient=\"index\")\n",
    "    features.drop(columns=[\"t1_name\", \"t2_name\"], inplace=True)\n",
    "    features.fillna(0,inplace=True)\n",
    "\n",
    "    # numerical_vars = [var for var in list(features.columns) if var[0] == \"t\" and var[2] == \"p\"]\n",
    "    numerical_vars = list(features.columns)\n",
    "    \n",
    "    labels = model_dataset[\"winner\"].astype(\"category\")\n",
    "    # print(features.shape, labels.shape)\n",
    "    xtrain, xtest, ytrain, ytest =  train_test_split( features, labels, random_state=seed, test_size=test_set_size)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xtrain[numerical_vars])\n",
    "\n",
    "    xtrain[numerical_vars] = scaler.transform(xtrain[numerical_vars]) #scale the training data\n",
    "    xtest[numerical_vars] = scaler.transform(xtest[numerical_vars]) #scale the testing data\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Initial Test of the Classification Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_test_winner_models():\n",
    "    xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(recent_matches_dataset ,seed=69)\n",
    "\n",
    "    basic_rank_pred = []\n",
    "    for index, row in xtest.iterrows():\n",
    "        basic_rank_pred.append(\"t1\" if row[\"t1_rank\"] >= row[\"t2_rank\"] else \"t2\")\n",
    "    print(f'The baseline predictor has a prediction accuracy of {round(accuracy_score(ytest,basic_rank_pred)*100,2)}%')\n",
    "\n",
    "    random.seed(69)\n",
    "    rand_pred = []\n",
    "    for index, row in xtest.iterrows():\n",
    "        rand_pred.append(\"t1\" if random.random() < 0.5 else \"t2\")\n",
    "    print(f'The random predictor has a prediction accuracy of {round(accuracy_score(ytest,rand_pred)*100,2)}%')\n",
    "\n",
    "    ratio_rank_pred = []\n",
    "    for index, row in xtest.iterrows():\n",
    "        ratio_rank_pred.append(\"t1\" if random.random() > float(row[\"t1_rank\"]) / float(row[\"t1_rank\"] + row[\"t2_rank\"]) else \"t2\")\n",
    "    print(f'The ratio rank predictor has a prediction accuracy of {round(accuracy_score(ytest,ratio_rank_pred)*100,2)}%')\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=15)\n",
    "    knn.fit(xtrain,ytrain)\n",
    "    print(f'The KNeighborsClassifier model has a prediction accuracy of {round(knn.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "    dtc = DecisionTreeClassifier(random_state=69)\n",
    "    dtc.fit(xtrain, ytrain)\n",
    "    print(f'The DecisionTreeClassifier model has a prediction accuracy of {round(dtc.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "    rf = RandomForestClassifier(criterion='entropy', n_estimators = 100, random_state = 69)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    print(f'The RandomForestClassifier model has a prediction accuracy of {round(rf.score(xtest, ytest)*100,2)}%')\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(xtrain, ytrain)\n",
    "    print(f'The GaussianNB model has a prediction accuracy of {round(nb.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "    svm = SVC()\n",
    "    svm.fit(xtrain, ytrain)\n",
    "    print(f'The SVC model has a prediction accuracy of {round(svm.score(xtest, ytest)*100,2)}%')\n",
    "    \n",
    "\n",
    "initial_test_winner_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Get Available Hyper-Parameters of the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## UNCOMMENT BELOW TO VIEW ALL TUNABLE PARAMS FOR THE VARIOUS MODELS ###############\n",
    "dummy_knn = KNeighborsClassifier()\n",
    "print(\"Available params for knn():\")\n",
    "print(\", \".join([f\"{key}: {dummy_knn.get_params()[key]}\" for key in dummy_knn.get_params()]))\n",
    "\n",
    "dummy_dtc = DecisionTreeClassifier()\n",
    "print(\"\\nAvailable params for dtc():\")\n",
    "print(\", \".join([f\"{key}: {dummy_dtc.get_params()[key]}\" for key in dummy_dtc.get_params()]))\n",
    "\n",
    "dummy_rf = RandomForestClassifier()\n",
    "print(\"\\nAvailable params for rf():\")\n",
    "print(\", \".join([f\"{key}: {dummy_rf.get_params()[key]}\" for key in dummy_rf.get_params()]))\n",
    "\n",
    "dummy_nb = GaussianNB()\n",
    "print(\"\\nAvailable params for nb():\")\n",
    "print(\", \".join([f\"{key}: {dummy_nb.get_params()[key]}\" for key in dummy_nb.get_params()]))\n",
    "\n",
    "dummy_svm = SVC()\n",
    "print(\"\\nAvailable params for svc():\")\n",
    "print(\", \".join([f\"{key}: {dummy_svm.get_params()[key]}\" for key in dummy_svm.get_params()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Performing Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_winner_models():\n",
    "\n",
    "    knn_tuning_results = {}\n",
    "    dtc_tuning_results =  {}\n",
    "    rf_tuning_results = {}\n",
    "    nb_tuning_results = {}\n",
    "    svm_tuning_results = {}\n",
    "\n",
    "    for daterange, part in dataset_partitions.iteritems():\n",
    "        print(daterange)\n",
    "      \n",
    "        # generate some random states !\n",
    "        random_states = [n + n**n for n in range(0,5)]\n",
    "        \n",
    "\n",
    "        xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(df=part, seed=1)\n",
    "\n",
    "        # xtrain, xtest, ytrain, ytest = convert_dataset_for_models(df=part, seed=seed)\n",
    "        knn_params = {\n",
    "            # \"n_neighbors\": np.arange(1, 100, 10),\n",
    "            \"n_neighbors\": [1] + [n for n in range(10,101,10)],\n",
    "            # \"random_state\" : random_states,\n",
    "        }\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_grid = GridSearchCV(estimator = knn, param_grid = knn_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        knn_grid.fit(xtrain, ytrain)    \n",
    "        knn_tuning_results[daterange] = dict(knn_grid.best_params_) \n",
    "        knn_tuning_results[daterange][\"score\"] = knn_grid.best_score_\n",
    "        print(\"knn: \", \", \".join([f\"{key} : {knn_grid.best_params_[key]}\" for key in knn_grid.best_params_]))\n",
    "\n",
    "\n",
    "        dtc_params = {\n",
    "            # \"n_neighbors\": np.arange(1, 100, 10),\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "        dtc = DecisionTreeClassifier()\n",
    "        dtc_grid = GridSearchCV(estimator = dtc, param_grid = dtc_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        dtc_grid.fit(xtrain, ytrain)    \n",
    "        dtc_tuning_results[daterange] = dict(dtc_grid.best_params_) \n",
    "        dtc_tuning_results[daterange][\"score\"] = dtc_grid.best_score_\n",
    "        print(\"dtc: \", \", \".join([f\"{key} : {dtc_grid.best_params_[key]}\" for key in dtc_grid.best_params_]))\n",
    "\n",
    "        rf_params = {\n",
    "            \"n_estimators\": [1] + [n for n in range(10,201,20)],\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        rf_grid = GridSearchCV(estimator = rf, param_grid = rf_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        rf_grid.fit(xtrain, ytrain)    \n",
    "        rf_tuning_results[daterange] = dict(rf_grid.best_params_) \n",
    "        rf_tuning_results[daterange][\"score\"] = rf_grid.best_score_\n",
    "        print(\"rf: \", \", \".join([f\"{key} : {rf_grid.best_params_[key]}\" for key in rf_grid.best_params_]))\n",
    "\n",
    "\n",
    "        nb_params = {\n",
    "            \"var_smoothing\": np.logspace(0,-9, num=10)\n",
    "        }\n",
    "        nb = GaussianNB()\n",
    "        nb_grid = GridSearchCV(estimator = nb, param_grid = nb_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        nb_grid.fit(xtrain, ytrain)    \n",
    "        nb_tuning_results[daterange] = dict(nb_grid.best_params_) \n",
    "        nb_tuning_results[daterange][\"score\"] = nb_grid.best_score_\n",
    "        print(\"nb: \", \", \".join([f\"{key} : {nb_grid.best_params_[key]}\" for key in nb_grid.best_params_]))\n",
    "\n",
    "        svm_params = {\n",
    "            'C': np.arange(8, 12) * 0.1,\n",
    "            'gamma': np.arange(1, 5) * 0.1,\n",
    "        }\n",
    "        svm = SVC()\n",
    "        svm_grid = GridSearchCV(estimator = svm, param_grid = svm_params, cv=5, return_train_score = True, scoring='accuracy')\n",
    "        svm_grid.fit(xtrain, ytrain)    \n",
    "        svm_tuning_results[daterange] = dict(svm_grid.best_params_) \n",
    "        svm_tuning_results[daterange][\"score\"] = abs(svm_grid.best_score_)\n",
    "        print(\"svm: \", \", \".join([f\"{key} : {svm_grid.best_params_[key]}\" for key in svm_grid.best_params_]))\n",
    "\n",
    "        # store the results after each iteration!\n",
    "        DataFrame.from_dict(knn_tuning_results, orient=\"index\").to_pickle(\"cache_knn_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(dtc_tuning_results, orient=\"index\").to_pickle(\"cache_dtc_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(rf_tuning_results, orient=\"index\").to_pickle(\"cache_rf_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(nb_tuning_results, orient=\"index\").to_pickle(\"cache_nb_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(svm_tuning_results, orient=\"index\").to_pickle(\"cache_svm_tuning_results.pkl\")\n",
    "\n",
    "    return knn_tuning_results, dtc_tuning_results, rf_tuning_results, nb_tuning_results, svm_tuning_results\n",
    "    \n",
    "# todo: uncomment this!\n",
    "# knn_tuning_results, dtc_tuning_results, rf_tuning_results, nb_tuning_results, svm_tuning_results = tune_winner_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Testing Classification Models After Hyper-Parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\My Drive\\Y3S\\DS 3000\\project\\ds3000-project\\notebook.ipynb Cell 23'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39m# test the classifier models after hyper-parameter tuning \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000026?line=1'>2</a>\u001b[0m knn_tuning_results_df : DataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39mcache/knn_tuning_results.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000026?line=2'>3</a>\u001b[0m dtc_tuning_results_df : DataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39mcache/dtc_tuning_results.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000026?line=3'>4</a>\u001b[0m rf_tuning_results_df : DataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39mcache/rf_tuning_results.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# test the classifier models after hyper-parameter tuning \n",
    "knn_tuning_results_df : DataFrame = pd.read_pickle(\"cache_knn_tuning_results.pkl\")\n",
    "dtc_tuning_results_df : DataFrame = pd.read_pickle(\"cache_dtc_tuning_results.pkl\")\n",
    "rf_tuning_results_df : DataFrame = pd.read_pickle(\"cache_rf_tuning_results.pkl\")\n",
    "nb_tuning_results_df : DataFrame = pd.read_pickle(\"cache_nb_tuning_results.pkl\")\n",
    "svm_tuning_results_df : DataFrame = pd.read_pickle(\"cache_svm_tuning_results.pkl\")\n",
    "\n",
    "print(\"KNN Tuning Results:\")\n",
    "print(knn_tuning_results_df.head(3))\n",
    "print(\"\\nDTC Tuning Results:\")\n",
    "print(dtc_tuning_results_df.head(3))\n",
    "print(\"\\nRF Tuning Results:\")\n",
    "print(rf_tuning_results_df.head(3))\n",
    "print(\"\\nNB Tuning Results:\")\n",
    "print(nb_tuning_results_df.head(3))\n",
    "print(\"\\nSVM Tuning Results:\")\n",
    "print(svm_tuning_results_df.head(3))\n",
    "\n",
    "models_best_results = pd.DataFrame(knn_tuning_results_df[\"score\"].rename(\"knn_score\"))\n",
    "models_best_results[\"dtc_score\"] = dtc_tuning_results_df[\"score\"]\n",
    "models_best_results[\"rf_score\"] = rf_tuning_results_df[\"score\"]\n",
    "models_best_results[\"nb_score\"] = nb_tuning_results_df[\"score\"]\n",
    "models_best_results[\"svm_score\"] = svm_tuning_results_df[\"score\"]\n",
    "print(models_best_results,\"\\n\\n\")\n",
    "\n",
    "models_best_results_stats = pd.DataFrame(knn_tuning_results_df[\"score\"].rename(\"knn_score\").describe())\n",
    "models_best_results_stats[\"dtc_score\"] = dtc_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats[\"rf_score\"] = rf_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats[\"nb_score\"] = nb_tuning_results_df[\"score\"].describe()\n",
    "models_best_results_stats[\"svm_score\"] = svm_tuning_results_df[\"score\"].describe()\n",
    "print(models_best_results_stats)\n",
    "\n",
    "def test_classifier_models_after_tuning() -> DataFrame:\n",
    "    # initialize the Dataframe so we can fill it in\n",
    "    results = DataFrame(\n",
    "        columns=[\"basic_rank_pred\", \"rand_pred\", \"ratio_rank_pred\", \"knn\", \"dtc\", \"rf\", \"nb\", \"svm\"],\n",
    "        index=dataset_partitions.index)\n",
    "\n",
    "    for part_daterange, part in dataset_partitions.iteritems():\n",
    "        xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(df=part, seed=1)\n",
    "\n",
    "        basic_rank_pred = []\n",
    "        for index, row in xtest.iterrows():\n",
    "            basic_rank_pred.append(\"t1\" if row[\"t1_rank\"] >= row[\"t2_rank\"] else \"t2\")\n",
    "        results.loc[part_daterange][\"basic_rank_pred\"] = accuracy_score(ytest,basic_rank_pred)\n",
    "\n",
    "        random.seed(69)\n",
    "        rand_pred = []\n",
    "        for index, row in xtest.iterrows():\n",
    "            rand_pred.append(\"t1\" if random.random() < 0.5 else \"t2\")\n",
    "        results.loc[part_daterange][\"rand_pred\"] = accuracy_score(ytest,rand_pred)\n",
    "\n",
    "        ratio_rank_pred = []\n",
    "        for index, row in xtest.iterrows():\n",
    "            ratio_rank_pred.append(\"t1\" if random.random() > float(row[\"t1_rank\"]) / float(row[\"t1_rank\"] + row[\"t2_rank\"]) else \"t2\")\n",
    "        results.loc[part_daterange][\"ratio_rank_pred\"] = accuracy_score(ytest,ratio_rank_pred)\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=int(knn_tuning_results_df.iloc[0][\"n_neighbors\"]))\n",
    "        knn.fit(xtrain,ytrain)\n",
    "        results.loc[part_daterange][\"knn\"] = knn.score(xtest, ytest)\n",
    "\n",
    "        dtc = DecisionTreeClassifier(random_state=int(dtc_tuning_results_df.iloc[0][\"random_state\"]))\n",
    "        dtc.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"dtc\"] = dtc.score(xtest, ytest)\n",
    "\n",
    "        rf = RandomForestClassifier(criterion='entropy', \n",
    "            n_estimators = int(rf_tuning_results_df.iloc[0][\"n_estimators\"]), \n",
    "            random_state = int(rf_tuning_results_df.iloc[0][\"random_state\"]))\n",
    "        rf.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"rf\"] = rf.score(xtest, ytest)\n",
    "\n",
    "        nb = GaussianNB(var_smoothing=nb_tuning_results_df.iloc[0][\"var_smoothing\"])\n",
    "        nb.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"nb\"] = nb.score(xtest, ytest)\n",
    "\n",
    "        svm = SVC(C=svm_tuning_results_df.iloc[0][\"C\"], gamma=svm_tuning_results_df.iloc[0][\"gamma\"])\n",
    "        svm.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"svm\"] = svm.score(xtest, ytest)\n",
    "\n",
    "        print(results.loc[part_daterange])\n",
    "    \n",
    "    return results\n",
    "\n",
    "classification_models_results = test_classifier_models_after_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_best_results.iloc[:, ::-1].plot(kind=\"barh\", figsize=(10,10), xlim=(.55,.67), width=.75, legend='reverse')\n",
    "plt.title(\"test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Prep For Decision Tree Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(df=part, seed=1)\n",
    "\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=69)\n",
    "dtc.fit(xtrain, ytrain)\n",
    "print(f'The DecisionTreeClassifier model has a prediction accuracy of {round(dtc.score(xtest, ytest)*100,2)}%')\n",
    "\n",
    "rf = RandomForestClassifier(criterion='entropy', n_estimators = 100, random_state = 69)\n",
    "rf.fit(xtrain, ytrain)\n",
    "print(f'The RandomForestClassifier model has a prediction accuracy of {round(rf.score(xtest, ytest)*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Graph of Feature Importances of Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "sorted_feat = rf.feature_importances_.argsort()\n",
    "plt.barh(xtrain.columns,rf.feature_importances_[sorted_feat])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Graph of Feature Importances of Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "sorted_feat = dtc.feature_importances_.argsort()\n",
    "plt.barh(xtrain.columns,dtc.feature_importances_[sorted_feat])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Visualization of a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "plot_tree(dtc, \n",
    "          feature_names=xtrain.columns,\n",
    "          class_names=[\"t1\", \"t2\"], \n",
    "          filled=True, impurity=True, #max_depth=5,\n",
    "          rounded=True)\n",
    "plt.title(\"Visual Representation of the Decision Tree for the DecisionTreeClassifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Visualization of a Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "plot_tree(rf.estimators_[0], \n",
    "          feature_names=xtrain.columns,\n",
    "          class_names=[\"t1\", \"t2\"], \n",
    "          filled=True, impurity=True, \n",
    "          rounded=True)\n",
    "plt.title(\"Visual Representation of one of the RandomForestClassifierDecision's Decision Trees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "\n",
    "# **ROUND WIN DIFF PREDICTOR MODELS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Converting Dataset for The Regression Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_for_reg_models(df : DataFrame, seed : int = 1, test_set_size : float = 0.3):\n",
    "    \n",
    "    teams_profiles : dict[str,Series] = make_teams_profiles(df, pstat_vars=SELECTED_PSTAT_VARS)\n",
    "\n",
    "    # convert all unranked teams to have rank of 31st position\n",
    "    model_dataset = df.copy()\n",
    "    model_dataset[\"t1_rank\"].fillna(31, inplace=True)\n",
    "    model_dataset[\"t2_rank\"].fillna(31, inplace=True)\n",
    "\n",
    "    # extract data for each team, extract player stats later\n",
    "    teams_stats = model_dataset.loc[:,[\"winner\",\"t1_rank\", \"t2_rank\", \"t1_name\", \"t2_name\"]]\n",
    "    # keep_dict = {}\n",
    "    # print(teams_stats.shape)\n",
    "    # add team player stat profiles to each entry / row of data in features dataframe\n",
    "    features_dict = {}\n",
    "    for index, row in teams_stats.iterrows():\n",
    "        t1_name, t2_name = row[\"t1_name\"], row[\"t2_name\"]\n",
    "        if t1_name in teams_profiles or t2_name in teams_profiles:\n",
    "            if not t1_name in teams_profiles:\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "                t1_profile = Series(list(teams_profiles[t2_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            elif not t2_name in teams_profiles:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t1_name].apply(lambda x: None).values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "            else:\n",
    "                t1_profile = Series(list(teams_profiles[t1_name].values), \n",
    "                    index = [f\"t1{pstat_var}\" for pstat_var in list(teams_profiles[t1_name].index)])\n",
    "                t2_profile = Series(list(teams_profiles[t2_name].values), \n",
    "                    index = [f\"t2{pstat_var}\" for pstat_var in list(teams_profiles[t2_name].index)])\n",
    "            features_dict[index] = pd.concat([row, t1_profile, t2_profile])\n",
    "    \n",
    "    features = DataFrame.from_dict(features_dict, orient=\"index\")\n",
    "    features.drop(columns=[\"t1_name\", \"t2_name\"], inplace=True)\n",
    "    features[\"winner\"] = features[\"winner\"].apply(lambda x: x == \"t1\")\n",
    "    features.fillna(0,inplace=True)\n",
    "    numerical_vars = list(features.columns)\n",
    "    labels = model_dataset[\"rw_diff\"].astype(\"int64\")\n",
    "\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest =  train_test_split( features, labels, random_state=seed, test_size=test_set_size)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xtrain[numerical_vars])\n",
    "    xtrain[numerical_vars] = scaler.transform(xtrain[numerical_vars]) #scale the training data\n",
    "    xtest[numerical_vars] = scaler.transform(xtest[numerical_vars]) #scale the testing data\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "# xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(recent_matches_dataset ,seed=69)\n",
    "# xtrain.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Initial Test of the Regression Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_test_rw_diff_models():\n",
    "    xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(dataset_partitions.iloc[1] ,seed=69)\n",
    "    # print(list(xtrain.columns))\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors=15)\n",
    "    knn.fit(xtrain,ytrain)\n",
    "    knn_ypred = knn.predict(xtest)\n",
    "    print(list(knn_ypred)[:10])\n",
    "    print(list(ytest)[:10])\n",
    "    print(f'The KNeighborsRegressor has a RMSE of {round(mean_squared_error(ytest, knn_ypred, squared=False),2)}')\n",
    "    print(f'The KNeighborsRegressor has a MAE of {round(mean_absolute_error(ytest, knn_ypred),2)}')\n",
    "\n",
    "\n",
    "    dtc = DecisionTreeRegressor(random_state=69)\n",
    "    dtc.fit(xtrain, ytrain)\n",
    "    dtc_ypred = dtc.predict(xtest)\n",
    "    print(f'The DecisionTreeRegressor has a RMSE of {round(mean_squared_error(ytest, dtc_ypred, squared=False),2)}')\n",
    "    print(f'The DecisionTreeRegressor has a MAE of {round(mean_absolute_error(ytest, dtc_ypred),2)}')\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(criterion='squared_error', n_estimators = 100, random_state = 69)\n",
    "    rf.fit(xtrain, ytrain)\n",
    "    rf_ypred   = rf.predict(xtest)\n",
    "    print(f'The RandomForestRegressor has a RMSE of {round(mean_squared_error(ytest, rf_ypred, squared=False),2)}')\n",
    "    print(f'The RandomForestRegressor has a MAE of {round(mean_absolute_error(ytest, rf_ypred),2)}')\n",
    "\n",
    "    svm = SVR()\n",
    "    svm.fit(xtrain, ytrain)\n",
    "    svm_ypred = svm.predict(xtest)\n",
    "    print(f'The SVR has a RMSE of {round(mean_squared_error(ytest, svm_ypred, squared=False),2)}')\n",
    "    print(f'The SVR has a MAE of {round(mean_absolute_error(ytest, svm_ypred),2)}')\n",
    "\n",
    "# initial_test_rw_diff_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Get Available Hyper-Parameters of the Regressors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_knn = KNeighborsRegressor()\n",
    "print(\"Available params for knn():\")\n",
    "print(\", \".join([f\"{key}: {dummy_knn.get_params()[key]}\" for key in dummy_knn.get_params()]))\n",
    "\n",
    "dummy_dtc = DecisionTreeRegressor()\n",
    "print(\"\\nAvailable params for dtc():\")\n",
    "print(\", \".join([f\"{key}: {dummy_dtc.get_params()[key]}\" for key in dummy_dtc.get_params()]))\n",
    "\n",
    "dummy_rf = RandomForestRegressor()\n",
    "print(\"\\nAvailable params for rf():\")\n",
    "print(\", \".join([f\"{key}: {dummy_rf.get_params()[key]}\" for key in dummy_rf.get_params()]))\n",
    "\n",
    "dummy_svm = SVR()\n",
    "print(\"\\nAvailable params for svr():\")\n",
    "print(\", \".join([f\"{key}: {dummy_svm.get_params()[key]}\" for key in dummy_svm.get_params()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Performing Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_rw_diff_models():\n",
    "    knn_reg_tuning_results = {}\n",
    "    dtr_reg_tuning_results =  {}\n",
    "    rf_reg_tuning_results = {}\n",
    "    svm_reg_tuning_results = {}\n",
    "\n",
    "    for daterange, part in dataset_partitions.iteritems():\n",
    "        print(daterange)\n",
    "        random_states = [n + n**n for n in range(0,5)]\n",
    "        xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(df=part, seed=1)\n",
    "\n",
    "        knn_params = {\n",
    "            \"n_neighbors\": [1] + [n for n in range(10,101,10)],\n",
    "        }\n",
    "        knn = KNeighborsRegressor()        \n",
    "        knn_grid = GridSearchCV(estimator = knn, param_grid = knn_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        knn_grid.fit(xtrain, ytrain)    \n",
    "        knn_reg_tuning_results[daterange] = dict(knn_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        knn_reg_tuning_results[daterange][\"score\"] = abs(knn_grid.best_score_)\n",
    "        \n",
    "        print(\"knn: \", \", \".join([f\"{key} : {knn_grid.best_params_[key]}\" for key in knn_grid.best_params_]))\n",
    "\n",
    "\n",
    "        dtc_params = {\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "        dtc = DecisionTreeRegressor()\n",
    "        dtc_grid = GridSearchCV(estimator = dtc, param_grid = dtc_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        dtc_grid.fit(xtrain, ytrain)    \n",
    "        dtr_reg_tuning_results[daterange] = dict(dtc_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        dtr_reg_tuning_results[daterange][\"score\"] = abs(dtc_grid.best_score_)\n",
    "\n",
    "        print(\"dtr: \", \", \".join([f\"{key} : {dtc_grid.best_params_[key]}\" for key in dtc_grid.best_params_]))\n",
    "\n",
    "        rf_params = {\n",
    "            \"n_estimators\": [n for n in range(20,201,20)],\n",
    "            \"random_state\" : random_states,\n",
    "        }\n",
    "        rf = RandomForestRegressor()\n",
    "        rf_grid = GridSearchCV(estimator = rf, param_grid = rf_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        rf_grid.fit(xtrain, ytrain)    \n",
    "        rf_reg_tuning_results[daterange] = dict(rf_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        rf_reg_tuning_results[daterange][\"score\"] = abs(rf_grid.best_score_)\n",
    "\n",
    "        print(\"rf: \", \", \".join([f\"{key} : {rf_grid.best_params_[key]}\" for key in rf_grid.best_params_]))\n",
    "            \n",
    "        svm_params = {\n",
    "            'C': np.arange(8, 12) * 0.1,\n",
    "            'gamma': [.1],\n",
    "        }\n",
    "        svm = SVR()\n",
    "        svm_grid = GridSearchCV(estimator = svm, param_grid = svm_params, cv=5, return_train_score = True, scoring='neg_mean_absolute_error')\n",
    "        svm_grid.fit(xtrain, ytrain)    \n",
    "        svm_reg_tuning_results[daterange] = dict(svm_grid.best_params_) \n",
    "        # take absolute value bc cvgridsearch uses negative values for optimization\n",
    "        svm_reg_tuning_results[daterange][\"score\"] = abs(svm_grid.best_score_)  \n",
    "        print(\"svm: \", \", \".join([f\"{key} : {svm_grid.best_params_[key]}\" for key in svm_grid.best_params_]))\n",
    "\n",
    "        # STORE RESULTS AFTER EACH PARTITION\n",
    "        DataFrame.from_dict(knn_reg_tuning_results, orient=\"index\").to_pickle(\"cache_knn_reg_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(dtr_reg_tuning_results, orient=\"index\").to_pickle(\"cache_dtr_reg_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(rf_reg_tuning_results, orient=\"index\").to_pickle(\"cache_rf_reg_tuning_results.pkl\")\n",
    "        DataFrame.from_dict(svm_reg_tuning_results, orient=\"index\").to_pickle(\"cache_svm_reg_tuning_results.pkl\")\n",
    "\n",
    "\n",
    "    return knn_reg_tuning_results, dtr_reg_tuning_results, rf_reg_tuning_results, svm_reg_tuning_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Testing Regression Models After Hyper-Parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Regression Tuning Results:\n",
      "                          n_neighbors     score\n",
      "2021-10-28 -> 2022-04-26           30  3.140531\n",
      "2021-05-01 -> 2021-10-28           30  3.203819\n",
      "2020-11-02 -> 2021-05-01           40  3.210087\n",
      "2020-05-06 -> 2020-11-02           20  3.332526\n",
      "2019-11-08 -> 2020-05-06           40  3.167512\n",
      "\n",
      "DTC Regression Tuning Results:\n",
      "                          random_state     score\n",
      "2021-10-28 -> 2022-04-26             1  3.581666\n",
      "2021-05-01 -> 2021-10-28             1  3.655842\n",
      "2020-11-02 -> 2021-05-01             2  3.588441\n",
      "2020-05-06 -> 2020-11-02           260  3.627666\n",
      "2019-11-08 -> 2020-05-06           260  3.600732\n",
      "\n",
      "RF Regression Tuning Results:\n",
      "                          n_estimators  random_state     score\n",
      "2021-10-28 -> 2022-04-26           160             6  2.934809\n",
      "2021-05-01 -> 2021-10-28           180             1  2.993379\n",
      "2020-11-02 -> 2021-05-01           120             2  2.918690\n",
      "2020-05-06 -> 2020-11-02           160             2  3.017105\n",
      "2019-11-08 -> 2020-05-06            60           260  2.961017\n",
      "\n",
      "SVM Regression Tuning Results:\n",
      "                            C  gamma     score\n",
      "2021-10-28 -> 2022-04-26  1.1    0.1  3.489168\n",
      "2021-05-01 -> 2021-10-28  1.1    0.1  3.451809\n",
      "2020-11-02 -> 2021-05-01  1.1    0.1  3.467474\n",
      "2020-05-06 -> 2020-11-02  1.1    0.1  3.515151\n",
      "2019-11-08 -> 2020-05-06  1.1    0.1  3.512185\n"
     ]
    }
   ],
   "source": [
    "knn_reg_tuning_results_df = pd.read_pickle(\"cache_knn_reg_tuning_results.pkl\")\n",
    "dtr_reg_tuning_results_df = pd.read_pickle(\"cache_dt_reg_tuning_results.pkl\")\n",
    "rf_reg_tuning_results_df = pd.read_pickle(\"cache_rf_reg_tuning_results.pkl\")\n",
    "svm_reg_tuning_results_df = pd.read_pickle(\"cache_svm_reg_tuning_results.pkl\")\n",
    "\n",
    "print(\"KNN Regression Tuning Results:\")\n",
    "print(knn_reg_tuning_results_df.head())\n",
    "print(\"\\nDTR Regression Tuning Results:\")\n",
    "print(dtr_reg_tuning_results_df.head())\n",
    "print(\"\\nRF Regression Tuning Results:\")\n",
    "print(rf_reg_tuning_results_df.head())\n",
    "print(\"\\nSVM Regression Tuning Results:\")\n",
    "print(svm_reg_tuning_results_df.head())\n",
    "\n",
    "reg_models_best_results = pd.DataFrame(knn_reg_tuning_results_df[\"score\"].rename(\"knn_score\"))\n",
    "reg_models_best_results[\"dtr_score\"] = dtr_reg_tuning_results_df[\"score\"]\n",
    "reg_models_best_results[\"rf_score\"] = rf_reg_tuning_results_df[\"score\"]\n",
    "reg_models_best_results[\"svm_score\"] = svm_reg_tuning_results_df[\"score\"]\n",
    "print(reg_models_best_results,\"\\n\\n\")\n",
    "\n",
    "reg_models_best_results_stats = pd.DataFrame(knn_reg_tuning_results_df[\"score\"].rename(\"knn_score\").describe())\n",
    "reg_models_best_results_stats[\"dtr_score\"] = dtr_reg_tuning_results_df[\"score\"].describe()\n",
    "reg_models_best_results_stats[\"rf_score\"] = rf_reg_tuning_results_df[\"score\"].describe()\n",
    "reg_models_best_results_stats[\"svm_score\"] = svm_reg_tuning_results_df[\"score\"].describe()\n",
    "print(reg_models_best_results_stats)\n",
    "\n",
    "\n",
    "def test_regression_models_after_tuning() -> DataFrame:\n",
    "    # initialize the Dataframe so we can fill it in\n",
    "    results = DataFrame(\n",
    "        columns=[\"basic_rank_pred\", \"rand_pred\", \"ratio_rank_pred\", \"knn\", \"dtr\", \"rf\", \"svm\"],\n",
    "        index=dataset_partitions.index)\n",
    "\n",
    "    for part_daterange, part in dataset_partitions.iteritems():\n",
    "        xtrain, xtest, ytrain, ytest = convert_dataset_for_reg_models(df=part, seed=1)\n",
    "\n",
    "        basic_rank_pred = []\n",
    "        for index, row in xtest.iterrows():\n",
    "            basic_rank_pred.append(\"t1\" if row[\"t1_rank\"] >= row[\"t2_rank\"] else \"t2\")\n",
    "        results.loc[part_daterange][\"basic_rank_pred\"] = accuracy_score(ytest,basic_rank_pred)\n",
    "\n",
    "        random.seed(69)\n",
    "        rand_pred = []\n",
    "        for index, row in xtest.iterrows():\n",
    "            rand_pred.append(\"t1\" if random.random() < 0.5 else \"t2\")\n",
    "        results.loc[part_daterange][\"rand_pred\"] = accuracy_score(ytest,rand_pred)\n",
    "\n",
    "        ratio_rank_pred = []\n",
    "        for index, row in xtest.iterrows():\n",
    "            ratio_rank_pred.append(\"t1\" if random.random() > float(row[\"t1_rank\"]) / float(row[\"t1_rank\"] + row[\"t2_rank\"]) else \"t2\")\n",
    "        results.loc[part_daterange][\"ratio_rank_pred\"] = accuracy_score(ytest,ratio_rank_pred)\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=int(knn_reg_tuning_results_df.iloc[0][\"n_neighbors\"]))\n",
    "        knn.fit(xtrain,ytrain)\n",
    "        results.loc[part_daterange][\"knn\"] = knn.score(xtest, ytest)\n",
    "\n",
    "        dtc = DecisionTreeRegressor(random_state=int(dtc_reg_tuning_results_df.iloc[0][\"random_state\"]))\n",
    "        dtc.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"dtr\"] = dtc.score(xtest, ytest)\n",
    "\n",
    "        rf = RandomForestClassifier(criterion='entropy', \n",
    "            n_estimators = int(rf_reg_tuning_results_df.iloc[0][\"n_estimators\"]), \n",
    "            random_state = int(rf_reg_tuning_results_df.iloc[0][\"random_state\"]))\n",
    "        rf.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"rf\"] = rf.score(xtest, ytest)\n",
    "\n",
    "        svm = SVR(\n",
    "            C=svm_reg_tuning_results_df.iloc[0][\"C\"], \n",
    "            gamma=svm_reg_tuning_results_df.iloc[0][\"gamma\"])\n",
    "        svm.fit(xtrain, ytrain)\n",
    "        results.loc[part_daterange][\"svm\"] = svm.score(xtest, ytest)\n",
    "\n",
    "        print(results.loc[part_daterange])\n",
    "    return results\n",
    "\n",
    "regression_models_results = test_regression_models_after_tuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_dataset_for_classification_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\My Drive\\Y3S\\DS 3000\\project\\ds3000-project\\notebook.ipynb Cell 42'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000051?line=0'>1</a>\u001b[0m \u001b[39m# testing best hyper paramters\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000051?line=1'>2</a>\u001b[0m xtrain, xtest, ytrain, ytest \u001b[39m=\u001b[39m convert_dataset_for_classification_models(df\u001b[39m=\u001b[39mrecent_matches_dataset, seed\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000051?line=2'>3</a>\u001b[0m knn \u001b[39m=\u001b[39m KNeighborsRegressor(n_neighbors\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/My%20Drive/Y3S/DS%203000/project/ds3000-project/notebook.ipynb#ch0000051?line=3'>4</a>\u001b[0m knn\u001b[39m.\u001b[39mfit(xtrain,ytrain)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'convert_dataset_for_classification_models' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg_tuning_results_df = pd.read_pickle(\"cache_knn_reg_tuning_results.pkl\")\n",
    "dtc_reg_tuning_results_df = pd.read_pickle(\"cache_dtc_reg_tuning_results.pkl\")\n",
    "rf_reg_tuning_results_df = pd.read_pickle(\"cache_rf_reg_tuning_results.pkl\")\n",
    "svm_reg_tuning_results_df = pd.read_pickle(\"cache_svm_reg_tuning_results.pkl\")\n",
    "\n",
    "print(\"KNN Regression Tuning Results:\")\n",
    "print(knn_reg_tuning_results_df.head())\n",
    "print(\"\\nDTC Regression Tuning Results:\")\n",
    "print(dtc_reg_tuning_results_df.head())\n",
    "print(\"\\nRF Regression Tuning Results:\")\n",
    "print(rf_reg_tuning_results_df.head())\n",
    "print(\"\\nSVM Regression Tuning Results:\")\n",
    "print(svm_reg_tuning_results_df.head())\n",
    "\n",
    "reg_models_best_results = pd.DataFrame(knn_reg_tuning_results_df[\"score\"].rename(\"knn_score\"))\n",
    "reg_models_best_results[\"dtc_score\"] = dtc_reg_tuning_results_df[\"score\"]\n",
    "reg_models_best_results[\"rf_score\"] = rf_reg_tuning_results_df[\"score\"]\n",
    "reg_models_best_results[\"svm_score\"] = svm_reg_tuning_results_df[\"score\"]\n",
    "print(reg_models_best_results,\"\\n\\n\")\n",
    "\n",
    "reg_models_best_results_stats = pd.DataFrame(knn_reg_tuning_results_df[\"score\"].rename(\"knn_score\").describe())\n",
    "reg_models_best_results_stats[\"dtc_score\"] = dtc_reg_tuning_results_df[\"score\"].describe()\n",
    "reg_models_best_results_stats[\"rf_score\"] = rf_reg_tuning_results_df[\"score\"].describe()\n",
    "reg_models_best_results_stats[\"svm_score\"] = svm_reg_tuning_results_df[\"score\"].describe()\n",
    "print(reg_models_best_results_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_models_best_results.iloc[:, ::-1].plot(kind=\"barh\", figsize=(15,10), xlim=(2.5,4), width=.75, legend='reverse')\n",
    "plt.title(\"test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = convert_dataset_for_classification_models(df=recent_matches_dataset, seed=1)\n",
    "\n",
    "print(dtc_reg_tuning_results_df[\"random_state\"].iloc[0])\n",
    "dtc = DecisionTreeRegressor(random_state=dtc_reg_tuning_results_df[\"random_state\"].iloc[0])\n",
    "dtc.fit(xtrain, ytrain)\n",
    "\n",
    "print(rf_reg_tuning_results_df[\"n_estimators\"].iloc[0], rf_reg_tuning_results_df[\"random_state\"].iloc[0])\n",
    "rf = RandomForestRegressor(criterion='entropy', \n",
    "    n_estimators = rf_reg_tuning_results_df[\"n_estimators\"].iloc[0], \n",
    "    random_state = rf_reg_tuning_results_df[\"random_state\"].iloc[0])\n",
    "rf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "sorted_feat = dtc.feature_importances_.argsort()\n",
    "plt.barh(xtrain.columns,dtc.feature_importances_[sorted_feat])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "sorted_feat = dtc.feature_importances_.argsort()\n",
    "plt.barh(xtrain.columns,dtc.feature_importances_[sorted_feat])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matches_dataset = pd.read_csv(\"all_matches_dataset.csv\")\n",
    "matches_dataset\n",
    "\n",
    "PLAYER_STAT_VARIABLES = [var.replace(\"t1p1_\",\"\") for var in list(matches_dataset.columns) if (\"t1p1_\" in var) and (not \"player\" in var)]\n",
    "print(\"All player stats variables:\", \", \".join(PLAYER_STAT_VARIABLES))\n",
    "\n",
    "many_pstat_vars = [\"kills\", \"assists\", \"deaths\", \"kdratio\", \"kddiff\", \"adr\", \"fkdiff\", \"rating\"]\n",
    "few_pstat_vars = [\"kills\", \"kdratio\", \"kddiff\", \"fkdiff\", \"rating\"]\n",
    "min_pstat_vars = [\"kills\", \"kdratio\", \"rating\"]\n",
    "\n",
    "SELECTED_PSTAT_VARS = min_pstat_vars\n",
    "print(\"SELECTED player stats variables:\", \", \".join(SELECTED_PSTAT_VARS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "# **Python Web Scrapers**\n",
    "\n",
    "**NOTE**: *This is in markdown because this is not meant to be run / executed. These scrapers take days to run fully and\n",
    "the results of doing so are already stored in the csv files* \n",
    "\n",
    "<br>\n",
    "\n",
    "----- \n",
    "## HLTV Matches URL Scraper:\n",
    "\n",
    "**crawler.py:**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from alive_progress import alive_bar, alive_it\n",
    "import os\n",
    "import csv \n",
    "import time\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "\n",
    "matches_urls = {}\n",
    "num_matches_loaded = 0\n",
    "\n",
    "def load_matches_urls_csv():\n",
    "    global matches_urls, num_matches_loaded\n",
    "    # load previously stored matches urls\n",
    "    with open(\"all_matches_urls.csv\", \"r\") as f:\n",
    "        matches_urls = {l.split(\",\")[0].strip():l.split(\",\")[1].strip() for l in f.readlines()[1:]}\n",
    "    num_matches_loaded = len(matches_urls)\n",
    "\n",
    "\n",
    "def format_hltv_date(hltv_date : str):\n",
    "    day, month, year = hltv_date.split(\"/\")\n",
    "    return \"-\".join((f\"20{year}\", month.rjust(2,'0'), day.rjust(2,'0')))\n",
    "\n",
    "def save_matches_urls_csv():\n",
    "    with open(\"all_matches_urls.csv\", \"w\") as f:\n",
    "        f.write(\"match_url,match_date\\n\")\n",
    "        for url in matches_urls:\n",
    "            f.write(f\"{url},{matches_urls[url]}\\n\")\n",
    "    \n",
    "def get_matches_urls(matches_page_html_text : str):\n",
    "    global matches_urls\n",
    "    match_list = []\n",
    "    while True:\n",
    "        soup = BeautifulSoup(matches_page_html_text, \"html.parser\")\n",
    "        match_list=soup.select(\"tr.group-1\") + soup.select(\"tr.group-2\")    \n",
    "        if len(match_list) == 50:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    # before = len(matches_urls)\n",
    "    for entry in match_list:\n",
    "        match_url = entry.select(\"td.date-col\")[0].select(\"a\")[0][\"href\"]\n",
    "        match_date = format_hltv_date(entry.select(\"td.date-col\")[0].select(\"a\")[0].select(\"div.time\")[0].text)\n",
    "        matches_urls[match_url] = match_date\n",
    "        \n",
    "def get_matches_page_html(offset : int = 0) -> str:\n",
    "    # round down to interval of 50\n",
    "    url=f\"https://www.hltv.org/stats/matches?offset={offset}\"\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def crawl(num_matches, starting_offset):\n",
    "    for query_offset in alive_it(range(0, num_matches, 50)):\n",
    "        if query_offset >= starting_offset:\n",
    "            html = get_matches_page_html(offset=query_offset)\n",
    "            get_matches_urls(html)\n",
    "            save_matches_urls_csv()\n",
    "            with open(\"crawler_last_page_no.txt\",\"w\") as f:\n",
    "                f.write(str(int(query_offset/50)))\n",
    "\n",
    "load_matches_urls_csv()\n",
    "\n",
    "# num_match_urls_to_get = input(\"How many total match urls do you want to fetch? \")\n",
    "# last_page_no = input(\"What page number did you get to last? \") \n",
    "# last_page_no = 0 if last_page_no == \"\" else int(last_page_no) \n",
    "\n",
    "num_match_urls_to_get = 80000\n",
    "with open(\"crawler_last_page_no.txt\",\"r\") as f:\n",
    "    last_page_no = int(f.read())\n",
    "\n",
    "# last_page_no = 0\n",
    "num_match_urls_to_get = 0\n",
    "\n",
    "if num_match_urls_to_get and (num_match_urls_to_get:=int(num_match_urls_to_get)) > 0:\n",
    "    crawl(num_match_urls_to_get, last_page_no*50)\n",
    "    print(f\"number of matches_urls: {num_matches_loaded} -> {len(matches_urls)}\")\n",
    "    save_matches_urls_csv()\n",
    "\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## HLTV Team Rankings Scraper\n",
    "\n",
    "**team_rankings_scraper.py:**\n",
    "```python\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## HLTV Matches Stats Scraper\n",
    "\n",
    "\n",
    "**matches_stats_scraper.py:**\n",
    "```python\n",
    "from re import S\n",
    "from urllib import request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from alive_progress import alive_bar, alive_it\n",
    "import os\n",
    "# import time\n",
    "from crawler import *\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "hltv_base_url = \"https://www.hltv.org\"\n",
    "\n",
    "match_stats_csv_fields = [\n",
    "    \"match_url\", \"match_date\", \"map\",\n",
    "\n",
    "    \"t1_name\", \"t1_id\", \"t1_total_rw\", \"t1_fh_rw\", \"t1_sh_rw\", \"t1_ot_rw\", \"t1_rating\", \"t1_fkills\", \"t1_cw\",\n",
    "\n",
    "    \"t1p1_player\", \"t1p1_kills\", \"t1p1_hskills\", \"t1p1_assists\", \"t1p1_fassists\", \"t1p1_deaths\", \"t1p1_kdratio\", \n",
    "    \"t1p1_kddiff\", \"t1p1_adr\", \"t1p1_fkdiff\", \"t1p1_rating\", \"t1p2_player\", \"t1p2_kills\", \"t1p2_hskills\", \n",
    "    \"t1p2_assists\", \"t1p2_fassists\", \"t1p2_deaths\", \"t1p2_kdratio\", \"t1p2_kddiff\", \"t1p2_adr\", \"t1p2_fkdiff\", \n",
    "    \"t1p2_rating\", \"t1p3_player\", \"t1p3_kills\", \"t1p3_hskills\", \"t1p3_assists\", \"t1p3_fassists\", \"t1p3_deaths\", \n",
    "    \"t1p3_kdratio\", \"t1p3_kddiff\", \"t1p3_adr\", \"t1p3_fkdiff\", \"t1p3_rating\", \"t1p4_player\", \"t1p4_kills\", \n",
    "    \"t1p4_hskills\", \"t1p4_assists\", \"t1p4_fassists\", \"t1p4_deaths\", \"t1p4_kdratio\", \"t1p4_kddiff\", \"t1p4_adr\", \n",
    "    \"t1p4_fkdiff\", \"t1p4_rating\", \"t1p5_player\", \"t1p5_kills\", \"t1p5_hskills\", \"t1p5_assists\", \"t1p5_fassists\", \n",
    "    \"t1p5_deaths\", \"t1p5_kdratio\", \"t1p5_kddiff\", \"t1p5_adr\", \"t1p5_fkdiff\", \"t1p5_rating\", \n",
    "\n",
    "    \"t2_name\", \"t2_id\", \"t2_total_rw\", \"t2_fh_rw\", \"t2_sh_rw\", \"t2_ot_rw\", \"t2_rating\", \"t2_fkills\", \"t2_cw\",\n",
    "    \n",
    "    \"t2p1_player\", \"t2p1_kills\", \"t2p1_hskills\", \"t2p1_assists\", \"t2p1_fassists\", \"t2p1_deaths\", \"t2p1_kdratio\", \n",
    "    \"t2p1_kddiff\", \"t2p1_adr\", \"t2p1_fkdiff\", \"t2p1_rating\", \"t2p2_player\", \"t2p2_kills\", \"t2p2_hskills\", \n",
    "    \"t2p2_assists\", \"t2p2_fassists\", \"t2p2_deaths\", \"t2p2_kdratio\", \"t2p2_kddiff\", \"t2p2_adr\", \"t2p2_fkdiff\", \n",
    "    \"t2p2_rating\", \"t2p3_player\", \"t2p3_kills\", \"t2p3_hskills\", \"t2p3_assists\", \"t2p3_fassists\", \"t2p3_deaths\", \n",
    "    \"t2p3_kdratio\", \"t2p3_kddiff\", \"t2p3_adr\", \"t2p3_fkdiff\", \"t2p3_rating\", \"t2p4_player\", \"t2p4_kills\", \n",
    "    \"t2p4_hskills\", \"t2p4_assists\", \"t2p4_fassists\", \"t2p4_deaths\", \"t2p4_kdratio\", \"t2p4_kddiff\", \"t2p4_adr\", \n",
    "    \"t2p4_fkdiff\", \"t2p4_rating\", \"t2p5_player\", \"t2p5_kills\", \"t2p5_hskills\", \"t2p5_assists\", \"t2p5_fassists\", \n",
    "    \"t2p5_deaths\", \"t2p5_kdratio\", \"t2p5_kddiff\", \"t2p5_adr\", \"t2p5_fkdiff\", \"t2p5_rating\"\n",
    "]\n",
    "\n",
    "def is_valid_match_page(html : str) -> bool:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return len(soup.select(\"table.stats-table\")) == 2\n",
    "\n",
    "def get_teams_stats_table(html : str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.select(\"table.stats-table\")[0], soup.select(\"table.stats-table\")[1]\n",
    "    \n",
    "# returns a list of 5 players stats which are flattened lists of (key, value) pairings (tuples)\n",
    "def get_player_stats(table):\n",
    "    result = {}\n",
    "    players = []\n",
    "\n",
    "    try:\n",
    "        result[\"team_name\"] = table.select(\"thead\")[0].select(\"th.st-teamname\")[0].text\n",
    "        columns = table.select(\"thead\")[0].select(\"th\")[1:]\n",
    "        rows = table.select(\"tbody\")[0].select(\"tr\")\n",
    "        for row in rows:\n",
    "            pstats = {}\n",
    "            stats = row.select(\"td\")\n",
    "            for stat in stats:\n",
    "                stat_name = stat[\"class\"][0][3:]\n",
    "                stat_value = (stat.text.split(\" \")[0]).strip()\n",
    "                pstats[stat_name] = stat_value\n",
    "\n",
    "                if stat_name == \"kills\":\n",
    "                    # get secondary stat in parenthesis and then trim off the parenthesis to keep just the value\n",
    "                    pstats[\"hskills\"] =  stat.text.split()[1][1:-1].strip() if \"(\" in stat.text else \"\"\n",
    "                if stat_name == \"assists\":\n",
    "                    # get secondary stat in parenthesis and then trim off the parenthesis to keep just the value\n",
    "                    pstats[\"fassists\"] = stat.text.split()[1][1:-1].strip() if \"(\" in stat.text else \"\"\n",
    "            players.append(pstats)\n",
    "    except:\n",
    "        print(table)\n",
    "    return players\n",
    "\n",
    "num_match_stats_loaded = 0\n",
    "completed_matches_urls = []\n",
    "match_stats = []\n",
    "\n",
    "last_scraped_match_stats = {}  # for debug purposes only\n",
    "\n",
    "\n",
    "def load_matches_stats_csv():\n",
    "    global completed_matches_urls, num_match_stats_loaded, match_stats\n",
    "    # load previously stored match stats\n",
    "    with open(\"all_matches_stats.csv\", \"r+\", encoding=\"utf-8\") as f:\n",
    "        if len(list(f.readlines())) <= 2:\n",
    "            print(\"EMPTY CSV\")\n",
    "            dictwriter = csv.DictWriter(f, fieldnames=match_stats_csv_fields)\n",
    "            dictwriter.writeheader()\n",
    "            return\n",
    "    with open(\"all_matches_stats.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        csvdictreader = csv.DictReader(f, delimiter=',') \n",
    "        for row in csvdictreader: \n",
    "            # print(len(row), len(match_stats_csv_fields))\n",
    "            if len(row) == len(match_stats_csv_fields):\n",
    "                if \"?rankingFilter=Top30\" in row[\"match_url\"]:\n",
    "                    row[\"match_url\"] = row[\"match_url\"].replace(\"?rankingFilter=Top30\",\"\")\n",
    "                match_stats.append(row)\n",
    "                if row[\"match_url\"] not in completed_matches_urls:\n",
    "                    completed_matches_urls.append(row[\"match_url\"])\n",
    "    num_match_stats_loaded = len(match_stats)\n",
    "    save_full_csv()\n",
    "\n",
    "def save_full_csv():\n",
    "    # try:\n",
    "    with open(\"all_matches_stats.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        dictwriter = csv.DictWriter(f, fieldnames=match_stats_csv_fields)\n",
    "        dictwriter.writeheader()\n",
    "        dictwriter.writerows(match_stats)\n",
    "\n",
    "def append_csv():\n",
    "    # try:\n",
    "    with open(\"all_matches_stats.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        dictwriter = csv.DictWriter(f, fieldnames=match_stats_csv_fields)\n",
    "        dictwriter.writerow(last_scraped_match_stats)\n",
    "\n",
    "def extract_map_team_stats(html : str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    team_stats_rows = soup.select(\"div.match-info-row .right\")\n",
    "    team_stats = {}\n",
    "    breakdown = team_stats_rows[0].text.replace(\")\",\"\").replace(\" \",\"\").split(\"(\")\n",
    "    team_stats[\"total_rw\"] = breakdown[0].split(\":\")  # total round wins\n",
    "    team_stats[\"fh_rw\"] = breakdown[1].split(\":\")  # first half round wins\n",
    "    team_stats[\"sh_rw\"] = breakdown[2].split(\":\")  # second half round wins\n",
    "    team_stats[\"ot_rw\"] = [\"0\",\"0\"] if len(breakdown) != 4 else breakdown[3].split(\":\")  # overtime round wins\n",
    "    team_stats[\"rating\"] = team_stats_rows[1].text.replace(\" \", \"\").split(\":\")  # team rating for the map\n",
    "    team_stats[\"fkills\"] = team_stats_rows[2].text.replace(\" \", \"\").split(\":\")  # first kills\n",
    "    team_stats[\"cw\"] = team_stats_rows[2].text.replace(\" \", \"\").split(\":\")  # clutches won\n",
    "    map_name = (txt:=soup.select(\"div.match-info-box\")[0].text.lower())[txt.find(\"map\"):].split()[1]\n",
    "    t1_stats = {f\"t1_{variable}\": team_stats[variable][0] for variable in team_stats}\n",
    "    t2_stats = {f\"t2_{variable}\": team_stats[variable][1] for variable in team_stats}\n",
    "    return t1_stats, t2_stats, map_name\n",
    "\n",
    "def extract_team_names_and_ids(html : str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    teams = {}\n",
    "    teams[\"t1_name\"] = soup.select(\"div.team-left a\")[0][\"href\"].split(\"/\")[-1].split(\"?\")[0]\n",
    "    teams[\"t1_id\"] = soup.select(\"div.team-left a\")[0][\"href\"].split(\"/\")[-2]\n",
    "    teams[\"t2_name\"] = soup.select(\"div.team-right a\")[0][\"href\"].split(\"/\")[-1].split(\"?\")[0]\n",
    "    teams[\"t2_id\"] = soup.select(\"div.team-right a\")[0][\"href\"].split(\"/\")[-2]\n",
    "    return teams\n",
    "\n",
    "def extract_stats(match_url : str, match_date : str):\n",
    "    stats = {\"match_url\": match_url, \"match_date\": match_date}\n",
    "    # if only given a uri, turn into full url\n",
    "    if not \"http\" in match_url:\n",
    "        match_url = hltv_base_url + match_url\n",
    "    r = None\n",
    "    while True:\n",
    "        r = requests.get(match_url)\n",
    "        if is_valid_match_page(r.text):\n",
    "            break\n",
    "        print(\"WE GOT RATE LIMITED D:\", end=\"\")\n",
    "        # print(r.text)\n",
    "        # input()\n",
    "        time.sleep(120)\n",
    "        print(\" trying again\")\n",
    "\n",
    "    tables = get_teams_stats_table(r.text)\n",
    "    team1_player_stats, team2_player_stats = get_player_stats(tables[0]) , get_player_stats(tables[1])\n",
    "\n",
    "    t1_pstats = {}\n",
    "    for pn in range(1,min(len(team1_player_stats)+1,6)):\n",
    "        pstats = team1_player_stats[pn-1]\n",
    "        for variable in pstats:\n",
    "            t1_pstats[f\"t1p{pn}_{variable}\"] = pstats[variable]\n",
    "\n",
    "    t2_pstats = {}\n",
    "    for pn in range(1,min(len(team2_player_stats)+1,6)):\n",
    "        pstats = team2_player_stats[pn-1]\n",
    "        for variable in pstats:\n",
    "            t2_pstats[f\"t2p{pn}_{variable}\"] = pstats[variable]\n",
    "\n",
    "    t1_map_stats, t2_map_stats, map_name = extract_map_team_stats(r.text)   \n",
    "    teams_info = extract_team_names_and_ids(r.text)\n",
    "\n",
    "    stats[\"map\"] = map_name\n",
    "    stats.update(t1_pstats)\n",
    "    stats.update(t2_pstats)\n",
    "    stats.update(t1_map_stats)\n",
    "    stats.update(t2_map_stats)\n",
    "    stats.update(teams_info)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def scrape_matches_pages():\n",
    "    global last_scraped_match_stats, match_stats, completed_matches_urls, num_matches_stats_scraped\n",
    "    num_matches_stats_to_scrape = input(\"How many matches stats do you want to scrape? \")\n",
    "\n",
    "    if num_matches_stats_to_scrape and (num_matches_stats_to_scrape:=int(num_matches_stats_to_scrape)) > 0:\n",
    "        load_matches_stats_csv()\n",
    "        num_matches_stats_scraped = 0\n",
    "        urls = set(matches_urls.keys())\n",
    "        with alive_bar(num_matches_stats_to_scrape, bar=\"filling\", length=30, title=\"Scraping... \") as bar:\n",
    "            for url in urls:\n",
    "                if num_matches_stats_scraped >= num_matches_stats_to_scrape:\n",
    "                    break\n",
    "                if not url in completed_matches_urls: \n",
    "                    if matches_urls[url] > \"2017-08-01\":\n",
    "                        match_stats.append(last_scraped_match_stats:=extract_stats(url, matches_urls[url]))\n",
    "                        completed_matches_urls.append(url)\n",
    "                        num_matches_stats_scraped += 1\n",
    "                        append_csv()\n",
    "                        bar() \n",
    "\n",
    "        print(f\"number of new/unique matches stats scraped: {num_match_stats_loaded} -> {len(match_stats)}\")\n",
    "\n",
    "\n",
    "scrape_matches_pages()\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
